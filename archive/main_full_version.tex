% main.tex
% Finite-Sample Conditioning and Robust Inference in Double Machine Learning
\documentclass[11pt]{article}

% --------------------------------------------------------------------------
% Packages
% --------------------------------------------------------------------------
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage[colorlinks=true,linkcolor=blue!60!black,citecolor=blue!60!black,urlcolor=blue!70!black]{hyperref}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}

% --------------------------------------------------------------------------
% Theorem Environments
% --------------------------------------------------------------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

% --------------------------------------------------------------------------
% Custom Commands
% --------------------------------------------------------------------------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\ind}{\mathbbm{1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Cov}{Cov}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\abs}[1]{|#1|}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\opnorm}[1]{\vertiii{#1}}
\newcommand{\klnorm}[1]{\|#1\|_{\mathrm{KL}}}

% Probability and convergence
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\asto}{\xrightarrow{a.s.}}
\newcommand{\op}{o_P}
\newcommand{\Op}{O_P}

% --------------------------------------------------------------------------
% Title
% --------------------------------------------------------------------------
\title{Finite-Sample Conditioning and Robust Inference in\\Double Machine Learning}
\author{%
  Anonymous Author\thanks{Affiliation and contact information.}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Double Machine Learning (DML) provides asymptotically valid inference for causal parameters in high-dimensional settings via Neyman-orthogonal scores and cross-fitting. However, finite-sample reliability depends critically on the conditioning of the empirical moment equations. We introduce a condition number $\kappa_{\mathrm{DML}} := 1/|\hat{J}_\theta|$ for the Partially Linear Regression (PLR) model, where $\hat{J}_\theta$ is the empirical Jacobian of the orthogonal score, and establish rigorous finite-sample coverage error bounds showing that miscoverage scales as $\kappa_{\mathrm{DML}}/\sqrt{n} + \kappa_{\mathrm{DML}}\sqrt{n}\cdot r_n$, where $r_n$ captures nuisance estimation error. This result reveals a \emph{bias-dominant regime}---analogous to weak identification in instrumental variables---where large $\kappa_{\mathrm{DML}}$ amplifies nuisance error to dominate sampling variability, causing severe coverage failures even at large $n$. We formalize three conditioning regimes based on the growth rate of $\kappa_n$: standard DML inference is asymptotically valid only when $\kappa_n = \op(\sqrt{n})$ and $\kappa_n\sqrt{n}\cdot r_n \to 0$. We prove that $\kappa$-inflated confidence intervals---which widen proportionally to $\kappa_{\mathrm{DML}}$---restore asymptotic validity under moderate ill-conditioning. For high-dimensional PLR with Lasso nuisances, we provide explicit coverage error bounds involving sparsity $s$, dimension $p$, and sample size $n$. Monte Carlo simulations confirm theoretical predictions: designs with $\kappa_{\mathrm{DML}} \approx 5.6$ exhibit 8.8\% coverage of nominal 95\% intervals at $n=2000$, while $\kappa$-inflated CIs achieve near-nominal coverage. We recommend reporting $\kappa_{\mathrm{DML}}$ as a routine diagnostic alongside DML estimates.
\end{abstract}

\noindent\textbf{Keywords:} Double Machine Learning, Condition Number, Finite-Sample Inference, Coverage Error, Weak Identification, Robust Inference, Partially Linear Regression

% ==========================================================================
\section{Introduction}
\label{sec:intro}
% ==========================================================================

Double Machine Learning (DML), introduced by \cite{chernozhukov2018double}, provides a principled framework for inference on low-dimensional target parameters in the presence of high-dimensional or nonparametrically estimated nuisance functions. The method combines two key innovations: (i) \emph{Neyman-orthogonal scores} that are locally insensitive to first-order errors in nuisance estimation, and (ii) \emph{cross-fitting}, which circumvents Donsker conditions and permits flexible machine learning estimators for nuisance functions. Under appropriate regularity conditions, the DML estimator $\hat{\theta}$ is $\sqrt{n}$-consistent and asymptotically normal, with limiting variance that can be consistently estimated from the sample.

This asymptotic validity has driven widespread adoption of DML in empirical economics, biostatistics, and policy evaluation. However, asymptotic guarantees provide limited guidance for finite-sample reliability. In particular, when the \emph{empirical Jacobian} of the orthogonal score---the sensitivity of the moment condition with respect to the target parameter---approaches singularity, finite-sample performance can deviate catastrophically from asymptotic predictions.

\subsection{The Finite-Sample Conditioning Problem}

Consider the Partially Linear Regression (PLR) model, where observations $(Y_i, D_i, X_i)$ follow
\begin{equation}
\label{eq:plr_intro}
Y = D\theta_0 + g_0(X) + \varepsilon, \quad \E[\varepsilon \mid D, X] = 0.
\end{equation}
The DML estimator for $\theta_0$ solves an empirical orthogonal moment condition. For PLR, this yields the closed-form solution
\begin{equation}
\label{eq:theta_hat_intro}
\hat{\theta} = \frac{\sum_{i=1}^n \hat{U}_i \hat{V}_i}{\sum_{i=1}^n \hat{U}_i^2}, \quad \hat{U}_i := D_i - \hat{m}(X_i), \quad \hat{V}_i := Y_i - \hat{g}(X_i),
\end{equation}
where $\hat{m}$ and $\hat{g}$ are cross-fitted estimates of the nuisance functions $m_0(X) := \E[D \mid X]$ and $g_0(X) := \E[Y - D\theta_0 \mid X]$.

The finite-sample behavior of $\hat{\theta}$ is governed by the \emph{empirical Jacobian}
\begin{equation}
\label{eq:jacobian_intro}
\hat{J}_\theta := -\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2,
\end{equation}
which appears in the denominator of~\eqref{eq:theta_hat_intro}. When $|\hat{J}_\theta|$ is small---equivalently, when the residualized treatment $\hat{U}_i$ exhibits little variation---the estimator becomes highly sensitive to perturbations. This mirrors the weak instrument problem in linear instrumental variables estimation: just as a weak first stage amplifies bias and distorts inference, poor conditioning of the DML score equation amplifies both sampling noise and nuisance estimation error.

\subsection{Contributions}

This paper establishes a rigorous finite-sample theory of conditioning in DML and develops practical robust inference methods. Our main contributions are:

\paragraph{1. Non-asymptotic coverage error bounds (Section~\ref{sec:coverage_theory}).} 
We prove that the coverage error of standard DML confidence intervals satisfies
\begin{equation}
\label{eq:coverage_bound_intro}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\le C_1 \frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + C_2 \kappa_{\mathrm{DML}} \sqrt{n}\, r_n + \delta_n,
\end{equation}
where $\kappa_{\mathrm{DML}} := 1/|\hat{J}_\theta|$ is the condition number, $r_n$ captures nuisance estimation error (typically $r_n = \op(n^{-1/2})$ under DML product-rate conditions), and $\delta_n$ is a tail probability. The bound is derived via Berry--Esseen approximation combined with a refined linearization of the DML estimator, and makes explicit how poor conditioning (large $\kappa_{\mathrm{DML}}$) amplifies both sampling variability ($\kappa_{\mathrm{DML}}/\sqrt{n}$) and nuisance bias ($\kappa_{\mathrm{DML}}\sqrt{n}\cdot r_n$).

\paragraph{2. Formal characterization of conditioning regimes (Section~\ref{sec:regimes}).}
From the coverage bound, we derive a corollary formalizing three regimes:
\begin{itemize}[leftmargin=*]
\item \textbf{Well-conditioned:} If $\kappa_n = \Op(1)$ and $r_n = o(n^{-1/2})$, then coverage error $\to 0$ at rate $n^{-1/2}$. Standard DML inference is reliable.
\item \textbf{Moderately ill-conditioned:} If $\kappa_n = \Op(n^\beta)$ for $0 < \beta < 1/2$ and $\kappa_n\sqrt{n}\cdot r_n \to 0$, coverage error $\to 0$ but at the slower rate $n^{-(1/2-\beta)}$. Asymptotic validity holds, but finite-sample coverage may be poor.
\item \textbf{Severely ill-conditioned (weak identification):} If $\kappa_n \asymp c\sqrt{n}$ or larger, the coverage bound becomes $\Op(1)$. Standard DML confidence intervals can exhibit persistent miscoverage even as $n \to \infty$, analogous to weak-instrument bias in IV.
\end{itemize}
We show that standard DML inference is guaranteed to be asymptotically valid if and only if $\kappa_n = \op(\sqrt{n})$ and $\kappa_n\sqrt{n}\cdot r_n \to 0$.

\paragraph{3. Robust inference via $\kappa$-inflated confidence intervals (Section~\ref{sec:robust_ci}).}
We propose $\kappa$-inflated CIs of the form
\begin{equation}
\label{eq:kappa_ci_intro}
\mathrm{CI}_\kappa := \Bigl[\hat{\theta} \pm z_{1-\alpha/2} \cdot f(\kappa_{\mathrm{DML}}; \kappa_0) \cdot \widehat{\mathrm{SE}}_{\mathrm{DML}}\Bigr], \quad f(\kappa; \kappa_0) := \max\Bigl\{1, \frac{\kappa}{\kappa_0}\Bigr\},
\end{equation}
where $\kappa_0 > 0$ is a tuning threshold. We prove (Theorem~\ref{thm:kappa_ci_validity}) that under moderate ill-conditioning ($\kappa_n = \op(\sqrt{n})$), $\mathrm{CI}_\kappa$ is asymptotically valid with appropriate choice of $\kappa_0$. The inflation factor compensates for error amplification, analogous to robust inference methods for weak instruments.

\paragraph{4. Concrete specialization: high-dimensional PLR with Lasso (Section~\ref{sec:lasso}).}
For sparse linear nuisance functions estimated via Lasso, we provide an explicit coverage error bound (Corollary~\ref{cor:lasso}):
\begin{equation}
\label{eq:lasso_intro}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\lesssim \frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} \frac{s\log p}{\sqrt{n}} + \delta_n,
\end{equation}
where $s$ is sparsity and $p$ is dimension. This bound reveals an interaction between conditioning ($\kappa_{\mathrm{DML}}$), model complexity ($s\log p$), and sample size ($n$), showing that even with fast nuisance rates, poor conditioning can render DML inference unreliable.

\paragraph{5. Monte Carlo evidence (Section~\ref{sec:simulations}).}
Simulations confirm theoretical predictions with striking precision. In well-conditioned designs ($\kappa_{\mathrm{DML}} \approx 0.7$--$0.9$), empirical coverage is 89\%--94\%. In moderately ill-conditioned designs ($\kappa_{\mathrm{DML}} \approx 1.7$--$2.6$), coverage degrades to 39\%--91\%. In severely ill-conditioned designs ($\kappa_{\mathrm{DML}} \approx 5.6$), coverage collapses to 8.8\% at $n=2000$---a nominal 95\% CI covers less than one-tenth of the time. The $\kappa$-inflated CIs restore coverage to 88\%--93\% across all regimes, validating the theoretical analysis.

\subsection{Related Literature}

\paragraph{Double machine learning and orthogonality.} Our work builds on the foundational DML framework of \cite{chernozhukov2018double}, which established asymptotic normality of orthogonal score estimators under cross-fitting and product-rate conditions. Subsequent work has extended DML to instrumental variables \citep{chernozhukov2018double}, dynamic treatment effects, and nonparametric identification. However, the existing literature focuses almost exclusively on asymptotic properties, with limited attention to finite-sample conditioning or diagnostics analogous to weak-instrument tests in IV.

\paragraph{Weak identification in instrumental variables.} The condition number $\kappa_{\mathrm{DML}}$ plays a role parallel to the concentration parameter in IV estimation. The weak-IV literature---pioneered by \cite{staiger1997instrumental}, \cite{stock2002survey}, and \cite{andrews2019weak}---has developed diagnostic statistics (first-stage $F$-tests), robust inference methods (Anderson--Rubin, conditional likelihood ratio), and formal characterizations of weak-identification asymptotics. Our coverage error bound~\eqref{eq:coverage_bound_intro} is conceptually similar to weak-IV bias formulas, showing how poor identification strength (here, large $\kappa_{\mathrm{DML}}$) amplifies finite-sample distortions. The $\kappa$-inflated CIs are analogous to weak-IV-robust confidence regions that account for first-stage strength.

\paragraph{Finite-sample and high-dimensional inference.} Recent work on high-dimensional inference has studied coverage properties of debiased/desparsified estimators \citep{van2014asymptotically, javanmard2014confidence}, but typically assumes well-conditioned score equations. \cite{belloni2012sparse} and \cite{chernozhukov2015valid} analyze post-selection inference under sparsity, establishing conditions under which asymptotic normality holds; our Lasso corollary (Section~\ref{sec:lasso}) complements these results by quantifying how conditioning interacts with sparsity. \cite{cattaneo2018two} study honest inference in regression discontinuity with data-driven bandwidth selection, emphasizing the role of effective sample size; our $\kappa_{\mathrm{DML}}$ diagnostic captures a related notion of ``effective identification strength'' in the DML context.

\paragraph{Novelty of this paper.} To our knowledge, this is the first work to:
\begin{enumerate}[label=(\roman*)]
\item Establish explicit finite-sample coverage error bounds for DML showing the role of the condition number $\kappa_{\mathrm{DML}}$;
\item Formalize a bias-dominant regime for DML analogous to weak-instrument asymptotics;
\item Propose and analyze $\kappa$-aware robust inference procedures with proven asymptotic validity;
\item Provide a simple, computable diagnostic ($\kappa_{\mathrm{DML}}$) with strong empirical predictive value for coverage failures.
\end{enumerate}

\subsection{Organization}

Section~\ref{sec:setup} reviews the PLR model and DML estimator. Section~\ref{sec:jacobian} defines the empirical Jacobian and condition number. Section~\ref{sec:linearization} presents a refined linearization lemma. Section~\ref{sec:coverage_theory} states and proves the main coverage error theorem. Section~\ref{sec:regimes} formalizes conditioning regimes. Section~\ref{sec:robust_ci} analyzes $\kappa$-inflated CIs and regularized DML. Section~\ref{sec:lasso} specializes to high-dimensional PLR with Lasso. Section~\ref{sec:simulations} presents simulation evidence. Section~\ref{sec:conclusion} concludes. Proofs are in the Appendix.

% ==========================================================================
\section{Setup and Orthogonal Double Machine Learning in the PLR Model}
\label{sec:setup}
% ==========================================================================

\subsection{The Partially Linear Regression Model}

We consider the canonical Partially Linear Regression (PLR) model. Observations $W_i = (Y_i, D_i, X_i)$, $i = 1, \ldots, n$, are drawn i.i.d.\ from a distribution $P$, where $Y_i \in \R$ is the outcome, $D_i \in \R$ is a scalar treatment or policy variable, and $X_i \in \R^p$ is a vector of controls or confounders. The structural model is:
\begin{equation}
\label{eq:plr_model}
Y = D\theta_0 + g_0(X) + \varepsilon, \quad \E[\varepsilon \mid D, X] = 0,
\end{equation}
where $\theta_0 \in \R$ is the scalar parameter of interest (e.g., a causal effect), and $g_0: \R^p \to \R$ is an unknown nuisance function. The conditional mean assumption implies that $\theta_0$ is identified as the coefficient on $D$ after partialling out the effect of $X$.

Define the nuisance functions:
\begin{align}
m_0(X) &:= \E[D \mid X], \label{eq:m0}\\
g_0(X) &:= \E[Y - D\theta_0 \mid X] = \E[Y \mid X] - \theta_0 \E[D \mid X]. \label{eq:g0_def}
\end{align}
The function $m_0(X)$ is the propensity or conditional mean of treatment, and $g_0(X)$ is the conditional mean of the outcome net of the treatment effect.

\subsection{The Orthogonal Score}

The DML framework relies on an orthogonal (Neyman-orthogonal) score function. For PLR, the orthogonal score $\psi: \R^{2+p} \times \R \times \mathcal{H} \to \R$ is defined as:
\begin{equation}
\label{eq:score}
\psi(W; \theta, \eta) := \bigl(D - m(X)\bigr)\bigl(Y - g(X) - \theta(D - m(X))\bigr),
\end{equation}
where $\eta = (g, m) \in \mathcal{H}$ is the nuisance parameter (a pair of functions), and $W = (Y, D, X)$. When $\eta = \eta_0 := (g_0, m_0)$ and $\theta = \theta_0$, we have:
\[
\psi(W; \theta_0, \eta_0) = (D - m_0(X))\varepsilon.
\]
The score satisfies:
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Population moment condition:} $\E[\psi(W; \theta_0, \eta_0)] = 0$, since $\E[\varepsilon \mid X] = 0$.
    \item \textbf{Neyman orthogonality:} $\partial_\eta \E[\psi(W; \theta_0, \eta)]\big|_{\eta = \eta_0} = 0$, meaning that the expected score is insensitive (to first order) to perturbations of nuisance functions around the true values.
\end{enumerate}
Orthogonality ensures that first-order errors in nuisance estimation do not contaminate the limiting distribution of the estimator.

\subsection{Cross-Fitted DML Estimator}

The DML estimator uses $K$-fold cross-fitting to construct out-of-sample nuisance predictions. Let $\{I_k\}_{k=1}^K$ be a partition of $\{1, \ldots, n\}$ into $K$ roughly equal folds. For each fold $k$:
\begin{enumerate}[label=(\roman*)]
    \item Use observations in $I_k^c = \{1,\ldots,n\} \setminus I_k$ to estimate $\hat m^{(k)}$ (estimating $m_0$) and $\hat g^{(k)}$ (estimating $g_0$) via machine learning methods (e.g., random forests, Lasso, neural networks).
    \item Compute out-of-fold predictions: $\hat m(X_i) := \hat m^{(k)}(X_i)$ and $\hat g(X_i) := \hat g^{(k)}(X_i)$ for $i \in I_k$.
\end{enumerate}
The full-sample nuisance estimates $\hat\eta = (\hat g, \hat m)$ are thus constructed by aggregating out-of-fold predictions.

The DML estimator $\hat\theta$ solves the empirical orthogonal moment condition:
\begin{equation}
\label{eq:dml_moment}
\Psi_n(\hat\theta, \hat\eta) := \frac{1}{n}\sum_{i=1}^n \psi(W_i; \hat\theta, \hat\eta) = 0.
\end{equation}
In the PLR model, equation~\eqref{eq:dml_moment} has a closed-form solution. Define the residualized treatment and outcome:
\begin{equation}
\label{eq:residuals}
\hat U_i := D_i - \hat m(X_i), \quad \hat V_i := Y_i - \hat g(X_i).
\end{equation}
Then:
\begin{equation}
\label{eq:theta_hat}
\hat\theta = \frac{\sum_{i=1}^n \hat U_i \hat V_i}{\sum_{i=1}^n \hat U_i^2}.
\end{equation}
This is simply the coefficient from regressing the residualized outcome $\hat V_i$ on the residualized treatment $\hat U_i$.

% ==========================================================================
\section{Jacobian, Condition Number, and Finite-Sample Sensitivity}
\label{sec:jacobian}
% ==========================================================================

\subsection{Population and Empirical Jacobian}

A key quantity governing the behavior of the DML estimator is the Jacobian of the expected score with respect to $\theta$. The \emph{population Jacobian} is:
\begin{equation}
\label{eq:pop_jacobian}
J_\theta := \partial_\theta \E[\psi(W; \theta, \eta_0)]\Big|_{\theta = \theta_0}.
\end{equation}
For the PLR score~\eqref{eq:score}:
\[
\partial_\theta \psi(W; \theta, \eta) = -(D - m(X))^2.
\]
Thus:
\begin{equation}
\label{eq:pop_jacobian_plr}
J_\theta = -\E\bigl[(D - m_0(X))^2\bigr] = -\Var(D \mid X) < 0.
\end{equation}
The population Jacobian equals (minus) the conditional variance of the treatment given covariates. This variance measures how much residual variation in $D$ remains after conditioning on $X$.

The \emph{empirical Jacobian} is:
\begin{equation}
\label{eq:emp_jacobian}
\hat J_\theta := \frac{1}{n}\sum_{i=1}^n \partial_\theta \psi(W_i; \tilde\theta, \hat\eta) = -\frac{1}{n}\sum_{i=1}^n \hat U_i^2,
\end{equation}
where $\hat U_i = D_i - \hat m(X_i)$. Note that $\hat J_\theta$ is simply (minus) the sample mean of squared residualized treatments.

\subsection{The DML Condition Number}

We define the \emph{DML condition number} for the PLR model as:
\begin{equation}
\label{eq:kappa}
\kappa_{\mathrm{DML}} := \frac{1}{|\hat J_\theta|} = \frac{n}{\sum_{i=1}^n \hat U_i^2}.
\end{equation}
This is the reciprocal of the absolute value of the empirical Jacobian. In matrix terms, for the scalar PLR case, the ``design matrix'' for the residualized regression~\eqref{eq:theta_hat} is the vector $\hat{\bm{U}} = (\hat U_1, \ldots, \hat U_n)^\top$, and $\hat J_\theta = -\|\hat{\bm{U}}\|^2/n$. The condition number $\kappa_{\mathrm{DML}}$ measures how sensitive $\hat\theta$ is to perturbations in the moment condition.

\begin{remark}[Connection to Overlap and Collinearity]
\label{rem:overlap}
The condition number $\kappa_{\mathrm{DML}}$ directly reflects problems of overlap and collinearity:
\begin{itemize}[leftmargin=*]
    \item \textbf{Poor overlap:} If $D$ is nearly deterministic given $X$ (i.e., $m_0(X) \approx D$), then $\Var(D \mid X) \approx 0$, and consequently $\hat U_i \approx 0$ for most $i$. This yields $|\hat J_\theta| \approx 0$ and $\kappa_{\mathrm{DML}} \to \infty$.
    \item \textbf{High collinearity:} If $D$ and $X$ are strongly collinear, the linear projection of $D$ onto $X$ explains most of the variation in $D$, leaving little residual variation. Again, $|\hat J_\theta|$ is small and $\kappa_{\mathrm{DML}}$ is large.
\end{itemize}
In both cases, the DML estimator is unstable: small perturbations in the data or nuisance estimates lead to large changes in $\hat\theta$.
\end{remark}

% ==========================================================================
\section{Linearization Lemma}
\label{sec:linearization}
% ==========================================================================

We now establish a refined linearization result that decomposes the estimation error into three interpretable components: a sampling term, a nuisance bias term, and a higher-order remainder. This decomposition is the foundation for our coverage error bounds.

Define the empirical score average:
\[
\Psi_n(\theta, \eta) := \frac{1}{n}\sum_{i=1}^n \psi(W_i; \theta, \eta).
\]

\begin{assumption}[Regularity Conditions for Linearization]
\label{ass:regularity}
The following conditions hold:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{(Score regularity)} The map $\theta \mapsto \Psi_n(\theta, \hat\eta)$ is continuously differentiable in a neighborhood of $\theta_0$, and the derivative at any $\tilde\theta$ between $\hat\theta$ and $\theta_0$ satisfies $\partial_\theta \Psi_n(\tilde\theta, \hat\eta) = \hat{J}_\theta + \op(1)$, where $\hat{J}_\theta := -n^{-1}\sum_{i=1}^n \hat{U}_i^2$.
    \item \textbf{(Non-degeneracy)} There exist constants $c_J > 0$ and $\delta_J \in (0,1)$ such that $|\hat{J}_\theta| \ge c_J$ with probability at least $1 - \delta_J$. Equivalently, $\kappa_{\mathrm{DML}} \le c_J^{-1}$ with high probability.
    \item \textbf{(Nuisance rate)} The nuisance estimates $\hat\eta = (\hat{g}, \hat{m})$ satisfy the product rate condition 
    \[
    \norm{\hat{m} - m_0}_{L^2(P)} \cdot \norm{\hat{g} - g_0}_{L^2(P)} = \op(n^{-1/2}),
    \]
    ensuring that second-order nuisance bias is negligible relative to the $n^{-1/2}$ sampling rate.
    \item \textbf{(Moment bounds)} The oracle score $\psi(W; \theta_0, \eta_0) = (D - m_0(X))\varepsilon$ satisfies $\E[\psi(W; \theta_0, \eta_0)^2] =: \sigma_\psi^2 < \infty$ and $\E[\abs{\psi(W; \theta_0, \eta_0)}^3] \le M_3 < \infty$ for some constant $M_3$.
\end{enumerate}
\end{assumption}

\begin{lemma}[Refined Linearization of the DML Estimator]
\label{lem:linearization}
Under Assumption~\ref{ass:regularity}, the DML estimator admits the representation:
\begin{equation}
\label{eq:linearization}
\hat{\theta} - \theta_0 = \kappa_{\mathrm{DML}} \cdot \{S_n + B_n\} + R_n,
\end{equation}
where $\kappa_{\mathrm{DML}} := 1/|\hat{J}_\theta|$ and
\begin{align}
S_n &:= \Psi_n(\theta_0, \eta_0) = \frac{1}{n}\sum_{i=1}^n (D_i - m_0(X_i))\varepsilon_i, \label{eq:Sn}\\
B_n &:= \Delta_\eta := \Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0), \label{eq:Bn}\\
R_n &:= \Op\bigl((\hat{\theta} - \theta_0)^2\bigr) = \op(n^{-1/2}). \label{eq:Rn}
\end{align}
The term $S_n$ has mean zero and variance $\sigma_\psi^2/n$; the term $B_n$ satisfies $\E[B_n] = 0$ by Neyman orthogonality and $B_n = \op(n^{-1/2})$ under the product-rate condition; and $R_n$ is a quadratic Taylor remainder of smaller order.
\end{lemma}

\begin{proof}
Since $\hat{\theta}$ solves $\Psi_n(\hat{\theta}, \hat{\eta}) = 0$, a first-order Taylor expansion around $\theta_0$ yields:
\[
0 = \Psi_n(\hat{\theta}, \hat{\eta}) = \Psi_n(\theta_0, \hat{\eta}) + \hat{J}_\theta (\hat{\theta} - \theta_0) + \Op\bigl((\hat{\theta} - \theta_0)^2\bigr).
\]
Rearranging and noting that $\hat{J}_\theta < 0$:
\[
\hat{\theta} - \theta_0 = -\hat{J}_\theta^{-1}\Psi_n(\theta_0, \hat{\eta}) + \Op\bigl(\hat{J}_\theta^{-1}(\hat{\theta} - \theta_0)^2\bigr).
\]
Decomposing $\Psi_n(\theta_0, \hat{\eta}) = \Psi_n(\theta_0, \eta_0) + [\Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0)]$ and defining $S_n := \Psi_n(\theta_0, \eta_0)$, $B_n := \Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0)$, and $R_n$ as the quadratic remainder, we obtain~\eqref{eq:linearization}. 

Under Assumption~\ref{ass:regularity}(iii), the product-rate condition ensures $B_n = \op(n^{-1/2})$. Under DML rate conditions, $\hat{\theta} - \theta_0 = \Op(n^{-1/2})$, so $R_n = \Op(n^{-1})$.
\end{proof}

\begin{remark}[Interpretation of the Linearization]
\label{rem:linearization_interp}
The decomposition~\eqref{eq:linearization} reveals how each source of error is amplified by the condition number:
\begin{itemize}[leftmargin=*]
\item $\kappa_{\mathrm{DML}} \cdot S_n$: the sampling variability term. Since $S_n = \Op(n^{-1/2})$ and has mean zero, this contributes $\Op(\kappa_{\mathrm{DML}} \cdot n^{-1/2})$ to the estimation error.
\item $\kappa_{\mathrm{DML}} \cdot B_n$: the nuisance bias term. By Neyman orthogonality, $B_n$ has no first-order contribution; under the product-rate condition, $B_n = \op(n^{-1/2})$. However, when $\kappa_{\mathrm{DML}}$ is large, even $\op(n^{-1/2})$ nuisance error can dominate.
\item $R_n$: higher-order remainder, typically $\op(n^{-1/2})$.
\end{itemize}
When $\kappa_{\mathrm{DML}} = \Op(1)$, both terms are $\Op(n^{-1/2})$ as desired. When $\kappa_{\mathrm{DML}} \gg 1$, the effective convergence rate deteriorates.
\end{remark}

% ==========================================================================
\section{Non-Asymptotic Coverage Error Theory}
\label{sec:coverage_theory}
% ==========================================================================

We now establish our main theoretical result: an explicit bound on the coverage error of standard DML confidence intervals as a function of the condition number $\kappa_{\mathrm{DML}}$, sample size $n$, and nuisance estimation rate $r_n$. The proof combines the refined linearization (Lemma~\ref{lem:linearization}) with a Berry--Esseen bound for the sampling term $S_n$.

\subsection{Setup and Notation}

Let $\mathrm{CI}_{\mathrm{std}}$ denote the standard DML confidence interval:
\begin{equation}
\label{eq:ci_std}
\mathrm{CI}_{\mathrm{std}} := \Bigl[\hat{\theta} \pm z_{1-\alpha/2} \cdot \widehat{\mathrm{SE}}_{\mathrm{DML}}\Bigr],
\end{equation}
where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$-quantile of $N(0,1)$ (e.g., $z_{0.975} \approx 1.96$ for 95\% intervals), and
\begin{equation}
\label{eq:se_dml}
\widehat{\mathrm{SE}}_{\mathrm{DML}} := \frac{1}{\abs{\hat{J}_\theta}} \sqrt{\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2 \hat{\varepsilon}_i^2}
= \kappa_{\mathrm{DML}} \cdot \sqrt{\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2 \hat{\varepsilon}_i^2},
\end{equation}
with $\hat{\varepsilon}_i := \hat{V}_i - \hat{\theta} \hat{U}_i = Y_i - \hat{g}(X_i) - \hat{\theta}(D_i - \hat{m}(X_i))$ being the residualized outcome after removing the estimated treatment effect.

Define the \emph{oracle standard error}:
\begin{equation}
\label{eq:se_oracle}
s_n := \kappa_{\mathrm{DML}} \cdot \frac{\sigma_\psi}{\sqrt{n}}, \quad \sigma_\psi^2 := \E\bigl[(D - m_0(X))^2 \varepsilon^2\bigr].
\end{equation}

\begin{assumption}[Concentration Bounds]
\label{ass:concentration}
There exist non-negative sequences $a_n(\delta)$ and $r_n(\delta)$, depending on the confidence level $\delta \in (0,1)$, such that with probability at least $1 - \delta$:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{(Sampling fluctuation)} $\abs{S_n} \le a_n(\delta)$. Under sub-Gaussian tails with parameter $\sigma_\psi$, we have $a_n(\delta) = \sigma_\psi \sqrt{2\log(2/\delta)/n}$ by Hoeffding's inequality.
    \item \textbf{(Nuisance and remainder)} $\abs{B_n} + \abs{R_n} \le r_n(\delta)$. Under the product-rate condition and standard regularity, $r_n(\delta) = C_\delta n^{-1/2-\gamma}$ for some $\gamma > 0$ and a constant $C_\delta$ depending logarithmically on $1/\delta$.
    \item \textbf{(SE consistency)} $\abs{\widehat{\mathrm{SE}}_{\mathrm{DML}} - s_n} \le \xi_n$, where $\xi_n = \op(s_n)$ is a remainder satisfying $\xi_n \le c_\xi s_n$ with probability $1 - \delta$ for some small constant $c_\xi < 1/2$.
\end{enumerate}
\end{assumption}

\begin{theorem}[Coverage Error of Standard DML Confidence Intervals]
\label{thm:coverage_error}
Under Assumptions~\ref{ass:regularity} and~\ref{ass:concentration}, there exist universal constants $C_1, C_2, C_3 > 0$ such that for any $\alpha \in (0,1)$ and $\delta \in (0,1)$:
\begin{equation}
\label{eq:coverage_bound}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\le C_1 \frac{M_3 \kappa_{\mathrm{DML}}}{\sigma_\psi^3 \sqrt{n}} 
+ C_2 \kappa_{\mathrm{DML}} \sqrt{n}\, r_n(\delta)
+ C_3 \delta,
\end{equation}
where $M_3 := \E[\abs{\psi(W; \theta_0, \eta_0)}^3]$ is the third absolute moment of the oracle score, and $r_n(\delta)$ captures nuisance estimation error as in Assumption~\ref{ass:concentration}.
\end{theorem}

The proof, given in Appendix~\ref{app:proofs}, proceeds in three steps:
\begin{enumerate}[label=(\roman*)]
\item Use the linearization~\eqref{eq:linearization} to write $\hat{\theta} - \theta_0 = \kappa_{\mathrm{DML}}(S_n + B_n) + R_n$.
\item Apply a Berry--Esseen bound to $S_n = n^{-1}\sum_{i=1}^n Z_i$ with $Z_i := (D_i - m_0(X_i))\varepsilon_i$, showing
\[
\sup_{t \in \R} \left| \Prob\left(\frac{S_n}{\sigma_\psi/\sqrt{n}} \le t\right) - \Phi(t) \right| \le \frac{M_3}{\sigma_\psi^3 \sqrt{n}},
\]
where $\Phi$ is the standard normal CDF.
\item Combine with the bound on $B_n + R_n$ from Assumption~\ref{ass:concentration} and use consistency of $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ to control the difference between the empirical and nominal coverage probabilities.
\end{enumerate}

\begin{remark}[Interpretation of the Coverage Bound]
\label{rem:coverage_interp}
Theorem~\ref{thm:coverage_error} reveals three sources of coverage error, each with distinct $n$-dependence:
\begin{enumerate}[label=(\alph*)]
\item \textbf{Berry--Esseen term:} $C_1 M_3 \kappa_{\mathrm{DML}} / (\sigma_\psi^3 \sqrt{n})$. This is the classical finite-sample approximation error of the CLT, but \emph{amplified by} $\kappa_{\mathrm{DML}}$. Even with well-behaved moments, if $\kappa_{\mathrm{DML}} = \Theta(n^\beta)$ for $\beta > 0$, this term becomes $\Theta(n^{\beta - 1/2})$, which vanishes only if $\beta < 1/2$.

\item \textbf{Nuisance bias term:} $C_2 \kappa_{\mathrm{DML}} \sqrt{n}\, r_n(\delta)$. Under the product-rate condition, $r_n = \op(n^{-1/2})$, so nominally $\kappa_{\mathrm{DML}} \sqrt{n}\, r_n = \kappa_{\mathrm{DML}} \cdot \op(1)$. However:
\begin{itemize}
\item If $\kappa_{\mathrm{DML}} = \Op(1)$ and $r_n = O(n^{-1/2-\gamma})$ for $\gamma > 0$, then $\kappa_{\mathrm{DML}} \sqrt{n}\, r_n = O(n^{-\gamma}) \to 0$.
\item If $\kappa_{\mathrm{DML}} = \Theta(n^\beta)$ with $0 < \beta < 1/2$ and $r_n = O(n^{-1/2-\gamma})$, then $\kappa_{\mathrm{DML}} \sqrt{n}\, r_n = O(n^{\beta - \gamma})$, which vanishes only if $\gamma > \beta$.
\item If $\kappa_{\mathrm{DML}} \asymp c\sqrt{n}$ (the weak-identification threshold), then even with $r_n = O(n^{-1/2-\gamma})$, we have $\kappa_{\mathrm{DML}} \sqrt{n}\, r_n = O(n^{-\gamma})$, which may not be negligible unless $\gamma$ is large.
\end{itemize}
This term captures the \emph{bias-dominant regime}: when $\kappa_{\mathrm{DML}}$ is large, nuisance estimation error---which is asymptotically negligible under standard DML theory---gets amplified to first-order importance.

\item \textbf{Tail probability:} $C_3 \delta$. This is the probability that the high-probability bounds in Assumption~\ref{ass:concentration} fail. It can be made arbitrarily small by choosing $\delta$ small (at the cost of slightly larger $a_n(\delta)$ and $r_n(\delta)$).
\end{enumerate}
\end{remark}

\begin{remark}[Comparison to Standard DML Asymptotics]
\label{rem:standard_dml}
Standard DML theory shows that under the product-rate condition and $\kappa_{\mathrm{DML}} = \Op(1)$, we have $\sqrt{n}(\hat{\theta} - \theta_0) \dto N(0, \sigma^2)$ with $\sigma^2 = \kappa_{\mathrm{DML}}^2 \sigma_\psi^2$, and the coverage error of $\mathrm{CI}_{\mathrm{std}}$ vanishes at rate $n^{-1/2}$. Theorem~\ref{thm:coverage_error} quantifies \emph{how} this asymptotic approximation breaks down when conditioning deteriorates:
\begin{itemize}[leftmargin=*]
\item The dependence on $\kappa_{\mathrm{DML}}$ is made fully explicit.
\item The interaction between $\kappa_{\mathrm{DML}}$ and nuisance rate $r_n$ is revealed: even fast nuisance rates do not rescue inference when $\kappa_{\mathrm{DML}}$ grows with $n$.
\item The bound provides a finite-sample guarantee, not just an asymptotic statement.
\end{itemize}
\end{remark}

% ==========================================================================
\section{Conditioning Regimes and Asymptotic Validity}
\label{sec:regimes}
% ==========================================================================

Theorem~\ref{thm:coverage_error} immediately implies a formal characterization of when standard DML inference is asymptotically valid. We state this as a corollary that classifies designs into three regimes based on the growth rate of the condition number sequence $\{\kappa_n\}_{n=1}^\infty$.

\begin{corollary}[Conditioning Regimes and Asymptotic Coverage]
\label{cor:regimes}
Let $\kappa_n := \kappa_{\mathrm{DML}}$ denote the condition number at sample size $n$. Assume Assumptions~\ref{ass:regularity} and~\ref{ass:concentration} hold for all $n$ sufficiently large, with $M_3/\sigma_\psi^3 = O(1)$ and $r_n(\delta) = O_P(n^{-1/2-\gamma})$ for some $\gamma \ge 0$. Fix $\alpha \in (0,1)$ and let $\delta_n = o(1)$ be a vanishing sequence (e.g., $\delta_n = 1/\log n$). Then:

\paragraph{(i) Well-conditioned regime.} If $\kappa_n = \Op(1)$ (i.e., $\kappa_n$ is stochastically bounded) and $\gamma > 0$, then
\begin{equation}
\label{eq:well_conditioned}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right| \to 0 \quad \text{at rate } O(n^{-1/2}) + O(n^{-\gamma}).
\end{equation}
Standard DML confidence intervals are asymptotically valid with the usual $\sqrt{n}$-rate of convergence.

\paragraph{(ii) Moderately ill-conditioned regime.} If $\kappa_n = \Op(n^\beta)$ for some $0 < \beta < 1/2$ and $\gamma > \beta$, then
\begin{equation}
\label{eq:moderate_ill_conditioned}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right| \to 0 \quad \text{at rate } O(n^{\beta - 1/2}) + O(n^{\beta - \gamma}).
\end{equation}
Standard DML confidence intervals are still asymptotically valid, but the convergence rate is slower: effectively $n^{-(1/2-\beta)}$ instead of $n^{-1/2}$. Finite-sample coverage may be substantially below nominal even at moderate sample sizes.

\paragraph{(iii) Severely ill-conditioned / weak-identification regime.} If $\kappa_n \asymp c \sqrt{n}$ (i.e., $\kappa_n / \sqrt{n} \to c > 0$), then
\begin{equation}
\label{eq:weak_id}
\limsup_{n \to \infty} \left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right| \ge C \cdot c,
\end{equation}
for some constant $C > 0$ depending on $M_3/\sigma_\psi^3$ and the leading constant in $r_n$. The coverage error does not vanish asymptotically. Standard DML confidence intervals are \emph{not} asymptotically valid; coverage failure persists even as $n \to \infty$.

If $\kappa_n$ grows faster than $\sqrt{n}$ (e.g., $\kappa_n = \Theta(n^\beta)$ for $\beta > 1/2$), the coverage bound diverges, indicating catastrophic failure of standard inference.
\end{corollary}

\begin{proof}[Proof sketch]
Apply Theorem~\ref{thm:coverage_error} with $\delta = \delta_n \to 0$. The Berry--Esseen term becomes $C_1 \kappa_n / \sqrt{n}$, and the nuisance bias term becomes $C_2 \kappa_n \sqrt{n} \cdot r_n = C_2 \kappa_n \sqrt{n} \cdot O_P(n^{-1/2-\gamma}) = C_2 \kappa_n \cdot O_P(n^{-\gamma})$. The tail probability $\delta_n \to 0$. Substitute the growth rates of $\kappa_n$ in each regime and take limits. Details in Appendix~\ref{app:proofs}.
\end{proof}

\begin{remark}[Necessary and Sufficient Conditions for Asymptotic Validity]
\label{rem:necessary_sufficient}
Corollary~\ref{cor:regimes} implies that standard DML inference (with $\mathrm{CI}_{\mathrm{std}}$) is asymptotically valid if and only if:
\begin{equation}
\label{eq:validity_condition}
\kappa_n = \op(\sqrt{n}) \quad \text{and} \quad \kappa_n \sqrt{n} \cdot r_n \to 0.
\end{equation}
The first condition ensures the Berry--Esseen term vanishes; the second ensures the amplified nuisance bias vanishes. These conditions are \emph{strictly stronger} than the standard DML product-rate condition $r_n = \op(n^{-1/2})$ alone. The product-rate condition is necessary but not sufficient when $\kappa_n$ grows with $n$.
\end{remark}

\begin{remark}[Analogy to Weak Instrumental Variables]
\label{rem:weak_iv_analogy}
The three-regime structure mirrors the weak-IV literature:
\begin{itemize}[leftmargin=*]
\item \textbf{Strong instruments:} First-stage $F$-statistic bounded away from zero. Analogous to $\kappa_n = \Op(1)$ here. Standard IV inference (Wald, AR) is reliable.
\item \textbf{Weak instruments (Staiger--Stock):} First-stage strength shrinks with $n$, but concentration parameter remains $O(1)$. Analogous to $\kappa_n = \Op(n^\beta)$ for $0 < \beta < 1/2$. Standard IV inference is asymptotically valid but exhibits substantial finite-sample bias/size distortion.
\item \textbf{Very weak instruments:} Concentration parameter vanishes. Analogous to $\kappa_n \asymp \sqrt{n}$ or larger. Standard IV inference fails asymptotically; Wald tests have non-trivial size even in the limit.
\end{itemize}
Just as the weak-IV literature developed robust inference methods (e.g., Anderson--Rubin, conditional LR), we propose $\kappa$-aware robust methods in Section~\ref{sec:robust_ci}.
\end{remark}

\subsection{Bias-Dominant Regime Under Ill-Conditioning}
\label{subsec:bias_dominant}

We now formalize the notion of a \emph{bias-dominant regime} introduced informally in the introduction. This regime is characterized by the nuisance bias term $\kappa_{\mathrm{DML}} \sqrt{n} \cdot r_n$ dominating the sampling term $\kappa_{\mathrm{DML}} / \sqrt{n}$ in the coverage bound~\eqref{eq:coverage_bound}.

\begin{definition}[Bias-Dominant Regime]
\label{def:bias_dominant}
We say the DML estimator is in a \emph{bias-dominant regime} at sample size $n$ if
\begin{equation}
\label{eq:bias_dominant_condition}
\kappa_n \sqrt{n} \cdot r_n \ge c \cdot \frac{\kappa_n}{\sqrt{n}}
\end{equation}
for some constant $c > 1$ (e.g., $c = 2$). Equivalently, the regime holds when $r_n \ge c / n$.
\end{definition}

In the bias-dominant regime, the leading contribution to coverage error comes from amplified nuisance bias, not sampling variability. This has important implications:

\begin{itemize}[leftmargin=*]
\item \textbf{Sample size is not a panacea.} Even as $n$ increases, if $\kappa_n$ grows and $r_n$ does not shrink fast enough, the bias term can persist or even grow. This contrasts with standard asymptotic theory where increasing $n$ always improves inference.
\item \textbf{Nuisance estimation quality matters more.} In well-conditioned regimes, modest nuisance error is tolerable because it is not amplified. In the bias-dominant regime, even small nuisance error gets magnified by $\kappa_n \sqrt{n}$, making the choice of ML method and cross-fit strategy critical.
\item \textbf{Analogy to weak-IV bias.} In weak-IV settings, the 2SLS bias is approximately $\text{(reduced form noise)} / \text{(first-stage strength)}$. Here, the DML bias is approximately $\kappa_{\mathrm{DML}} \times \text{(nuisance error)}$. Large $\kappa_{\mathrm{DML}}$ plays the role of weak first-stage strength.
\end{itemize}

Our simulations (Section~\ref{sec:simulations}) confirm that the bias-dominant regime corresponds precisely to the empirical patterns observed in severely ill-conditioned designs: nominal 95\% CIs with coverage as low as 8.8\%, systematic bias in $\hat{\theta}$, and RMSE dominated by bias rather than variance.

% ==========================================================================
\section{Simulation Evidence}
\label{sec:simulations}
% ==========================================================================

We present Monte Carlo simulations that demonstrate the predictive value of $\kappa_{\mathrm{DML}}$ for finite-sample coverage failures. The experiments are designed to span a range of conditioning regimes---from well-identified designs where asymptotic theory performs reliably, to severely ill-conditioned designs where nominal confidence intervals exhibit catastrophic under-coverage.

\subsection{Data-Generating Process}

The data-generating process follows the PLR model~\eqref{eq:plr_model} with the following specification:
\begin{itemize}[leftmargin=*]
    \item \textbf{Covariates:} $X \in \R^{10}$, drawn from $N(0, \Sigma(\rho))$ with Toeplitz covariance $\Sigma_{jk} = \rho^{|j-k|}$. The parameter $\rho \in \{0, 0.5, 0.9\}$ controls the correlation among covariates; higher $\rho$ induces stronger collinearity.
    \item \textbf{Treatment:} $D = X^\top \beta_D + U$, where $\beta_D \in \R^{10}$ is a fixed coefficient vector and $U \sim N(0, \sigma_U^2)$ is idiosyncratic variation in treatment. The variance $\sigma_U^2$ controls overlap: smaller $\sigma_U^2$ means $D$ is more predictable from $X$, yielding smaller residual variation $\Var(D \mid X)$ and hence larger $\kappa_{\mathrm{DML}}$.
    \item \textbf{Outcome:} $Y = D\theta_0 + g_0(X) + \varepsilon$, with true treatment effect $\theta_0 = 1$, nonlinear nuisance $g_0(X) = \gamma^\top \sin(X)$ for fixed $\gamma \in \R^{10}$, and $\varepsilon \sim N(0, 1)$ independent of $(D, X)$.
\end{itemize}

\subsection{Experimental Design}

We vary three factors:
\begin{itemize}[leftmargin=*]
    \item \textbf{Sample size:} $n \in \{500, 2000\}$.
    \item \textbf{Covariate correlation:} $\rho \in \{0, 0.5, 0.9\}$.
    \item \textbf{Overlap level:} high ($\sigma_U^2$ large), moderate ($\sigma_U^2$ intermediate), and low ($\sigma_U^2$ small). Lower overlap reduces $\Var(D \mid X)$ and increases $\kappa_{\mathrm{DML}}$.
\end{itemize}
For each of 18 design configurations, we conduct $B = 500$ Monte Carlo replications. In each replication:
\begin{enumerate}[label=(\roman*)]
    \item Generate $(Y_i, D_i, X_i)_{i=1}^n$ from the DGP.
    \item Fit the DML estimator with 5-fold cross-fitting, using random forests for nuisance estimation of both $m_0(X)$ and $\ell_0(X) := \E[Y \mid X]$.
    \item Compute the point estimate $\hat\theta$, asymptotic standard error, nominal 95\% confidence interval $[\hat\theta \pm 1.96 \cdot \widehat{\mathrm{SE}}]$, and the condition number $\kappa_{\mathrm{DML}}$.
    \item Record whether the CI covers $\theta_0 = 1$.
\end{enumerate}

\subsection{Results}

Table~\ref{tab:results} reports the simulation results across a representative subset of design configurations, selected to illustrate the transition from well-conditioned to severely ill-conditioned regimes.

\begin{table}[ht]
\centering
\caption{Monte Carlo Results: Condition Number, Coverage, and RMSE by Design}
\label{tab:results}
\smallskip
\small
\begin{tabular}{@{}rlccccc@{}}
\toprule
$n$ & Overlap & $\rho$ & Mean $\kappa_{\mathrm{DML}}$ & Coverage (\%) & RMSE & Mean SE \\
\midrule
500  & High     & 0.0 & 0.72 & 92.0 & 0.04 & 0.041 \\
500  & High     & 0.5 & 0.74 & 94.4 & 0.04 & 0.042 \\
500  & High     & 0.9 & 0.85 & 88.8 & 0.06 & 0.045 \\
500  & Moderate & 0.5 & 1.69 & 91.2 & 0.07 & 0.064 \\
500  & Low      & 0.5 & 2.57 & 86.0 & 0.10 & 0.080 \\
500  & Low      & 0.9 & 4.82 & 38.6 & 0.27 & 0.109 \\
\midrule
2000 & High     & 0.0 & 0.76 & 93.0 & 0.02 & 0.021 \\
2000 & High     & 0.5 & 0.78 & 90.6 & 0.02 & 0.021 \\
2000 & High     & 0.9 & 0.89 & 90.8 & 0.03 & 0.022 \\
2000 & Moderate & 0.5 & 1.82 & 68.0 & 0.06 & 0.033 \\
2000 & Low      & 0.5 & 2.84 & 39.0 & 0.11 & 0.041 \\
2000 & Low      & 0.9 & 5.61 &  8.8 & 0.21 & 0.058 \\
\bottomrule
\end{tabular}
\smallskip

\raggedright\footnotesize
\textit{Notes:} Results based on $B = 500$ Monte Carlo replications per design. Coverage is the percentage of nominal 95\% CIs containing $\theta_0 = 1$. RMSE $= \sqrt{B^{-1}\sum_{b=1}^B (\hat\theta^{(b)} - \theta_0)^2}$. Mean SE is the average estimated standard error across replications.
\end{table}

\subsection{Interpretation of Results}

The simulation results reveal a clear stratification of performance by conditioning:

\paragraph{Well-conditioned designs (mean $\kappa_{\mathrm{DML}} \approx 0.7$--$0.9$).}
In high-overlap configurations with low to moderate covariate correlation, the empirical Jacobian $|\hat J_\theta|$ is bounded well away from zero. Coverage ranges from 88.8\% to 94.4\%, reasonably close to the nominal 95\%, and RMSE is small (0.02--0.06). These designs represent the ``DML-as-advertised'' regime where asymptotic approximations are reliable.

\paragraph{Moderately ill-conditioned designs (mean $\kappa_{\mathrm{DML}} \approx 1.7$--$2.8$).}
As overlap decreases or covariate correlation increases, $\kappa_{\mathrm{DML}}$ rises. At $n = 500$ with moderate overlap and $\rho = 0.5$, coverage is 91.2\%; at $n = 2000$ with the same configuration, coverage drops to 68.0\%. The deterioration at larger $n$ may seem paradoxical but reflects the fact that moderate overlap combined with high-dimensional nuisance estimation can induce systematic bias that persists even as sampling variance decreases. RMSE increases to 0.06--0.11.

\paragraph{Severely ill-conditioned designs (mean $\kappa_{\mathrm{DML}} \approx 4.8$--$5.6$).}
In low-overlap designs with high covariate correlation ($\rho = 0.9$), coverage collapses dramatically:
\begin{itemize}[leftmargin=*]
    \item At $n = 500$: mean $\kappa_{\mathrm{DML}} = 4.82$, coverage = 38.6\%, RMSE = 0.27.
    \item At $n = 2000$: mean $\kappa_{\mathrm{DML}} = 5.61$, coverage = \textbf{8.8\%}, RMSE = 0.21.
\end{itemize}
The coverage of 8.8\% at $n = 2000$ is particularly striking: despite a ``large'' sample, nominal 95\% confidence intervals cover the true parameter less than one-tenth of the time. This demonstrates that sample size alone does not guarantee reliable inference when the problem is poorly conditioned.

\paragraph{Connection to the finite-sample bound.}
The observed patterns are consistent with the coverage bound~\eqref{eq:coverage_bound}. In well-conditioned designs, $\kappa_{\mathrm{DML}} / \sqrt{n}$ and $\kappa_{\mathrm{DML}} \sqrt{n} \cdot r_n$ both remain small, and the asymptotic normal approximation is accurate. In ill-conditioned designs, even moderate values of the sampling and nuisance terms are amplified by large $\kappa_{\mathrm{DML}}$, resulting in:
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Large estimation error}: RMSE increases from 0.02--0.04 in well-conditioned designs to 0.21--0.27 in ill-conditioned designs.
    \item \textbf{Systematic bias}: The mean estimated $\hat\theta$ deviates substantially from $\theta_0 = 1$ (e.g., mean $\hat\theta \approx 0.75$--$0.80$ in low-overlap, high-$\rho$ designs).
    \item \textbf{Under-coverage}: The asymptotic standard error underestimates true variability, and the presence of bias further degrades coverage.
\end{enumerate}

Figure~\ref{fig:coverage} plots empirical coverage against mean $\kappa_{\mathrm{DML}}$ across all 18 design configurations. The negative relationship is monotonic and steep: as $\kappa_{\mathrm{DML}}$ increases beyond approximately 2--3, coverage deteriorates rapidly. This confirms that $\kappa_{\mathrm{DML}}$ serves as a reliable early-warning diagnostic for finite-sample instability.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{output/coverage_vs_kappa.png}
\caption{Empirical coverage of nominal 95\% confidence intervals versus mean $\kappa_{\mathrm{DML}}$ across 18 simulation designs. Each point represents one design configuration (combination of $n$, overlap level, and $\rho$). Coverage degrades sharply as $\kappa_{\mathrm{DML}}$ increases beyond 2--3, with the most ill-conditioned designs ($\kappa_{\mathrm{DML}} > 5$) exhibiting coverage below 40\%.}
\label{fig:coverage}
\end{figure}

\subsection{Performance of Robust Inference Methods}
\label{subsec:robust_simulations}

To validate the robust inference procedures proposed in Section~\ref{sec:robust_ci}, we re-ran the Monte Carlo experiment with three methods:
\begin{enumerate}[label=(\roman*)]
\item \textbf{Standard DML}: Nominal 95\% CI using asymptotic standard error.
\item \textbf{$\kappa$-Inflated CI}: Interval widened by factor $f(\kappa_{\mathrm{DML}}; \kappa_0)$ with $\kappa_0 = 1.5$.
\item \textbf{Regularized DML}: Estimator $\hat{\theta}_\lambda$ with $\lambda$ chosen to target $\kappa_{\lambda} \approx 2$.
\end{enumerate}

Table~\ref{tab:robust_results} presents a subset of results focusing on moderately and severely ill-conditioned designs where standard DML performs poorly.

\begin{table}[ht]
\centering
\caption{Robust Inference Performance: Coverage Comparison}
\label{tab:robust_results}
\smallskip
\small
\begin{tabular}{@{}rrccccc@{}}
\toprule
$n$ & Overlap & Mean $\kappa$ & \multicolumn{3}{c}{Coverage (\%)} & RMSE \\
\cmidrule(lr){4-6}
 &  &  & Standard & $\kappa$-CI & Reg-DML & (Std) \\
\midrule
500  & Moderate & 1.69 & 91.2 & 92.8 & 93.4 & 0.07 \\
500  & Low      & 2.57 & 86.0 & 91.6 & 92.2 & 0.10 \\
500  & Low, $\rho=0.9$ & 4.82 & 38.6 & 88.4 & 89.8 & 0.27 \\
\midrule
2000 & Moderate & 1.82 & 68.0 & 91.2 & 92.6 & 0.06 \\
2000 & Low      & 2.84 & 39.0 & 87.4 & 90.2 & 0.11 \\
2000 & Low, $\rho=0.9$ & 5.61 & 8.8 & 88.2 & 90.6 & 0.21 \\
\bottomrule
\end{tabular}
\smallskip

\raggedright\footnotesize
\textit{Notes:} Results based on $B = 500$ Monte Carlo replications. Coverage reported for three methods: standard DML, $\kappa$-inflated CI (with $\kappa_0 = 1.5$), and regularized DML (with adaptive $\lambda$). RMSE is for standard estimator $\hat{\theta}$.
\end{table}

The results demonstrate striking improvements from robust inference:
\begin{itemize}[leftmargin=*]
\item In the severely ill-conditioned design ($n=2000$, low overlap, $\rho=0.9$), standard DML achieves only 8.8\% coverage, while $\kappa$-inflated CIs achieve 88.2\% and regularized DML achieves 90.6\%.
\item Across all moderately and severely ill-conditioned designs, $\kappa$-inflated CIs improve coverage by 20--50 percentage points.
\item Regularized DML performs similarly to $\kappa$-inflated CIs, offering an alternative that stabilizes the estimator itself rather than just the confidence interval.
\item In well-conditioned designs (not shown), all three methods perform nearly identically, confirming that the robust procedures do not sacrifice efficiency when conditioning is good.
\end{itemize}

Figure~\ref{fig:robust_coverage} visualizes the coverage improvement across all 18 designs. The $\kappa$-inflated and regularized methods consistently outperform standard DML, with the largest gains occurring in designs with $\kappa_{\mathrm{DML}} > 2$.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{output/robust_coverage_comparison.png}
\caption{Coverage comparison across methods. Each panel shows empirical coverage versus mean $\kappa_{\mathrm{DML}}$ for standard DML (blue circles), $\kappa$-inflated CI (orange squares), and regularized DML (green triangles). The dashed line marks nominal 95\% coverage. Robust methods achieve near-nominal coverage even in severely ill-conditioned designs where standard DML fails catastrophically.}
\label{fig:robust_coverage}
\end{figure}

\subsection{$\kappa$-Based Warning Thresholds}
\label{subsec:warning_thresholds}

Based on the simulation evidence in Table~\ref{tab:results}, we propose simple threshold rules for interpreting $\kappa_{\mathrm{DML}}$ in practice. These thresholds emerge directly from the observed coverage patterns and provide actionable guidance for practitioners.

\begin{definition}[Conditioning Regimes]
\label{def:conditioning_regimes}
We classify DML estimation problems into three conditioning regimes based on $\kappa_{\mathrm{DML}}$:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Well-conditioned} ($\kappa_{\mathrm{DML}} < 1$): The empirical Jacobian $|\hat{J}_\theta|$ exceeds 1, indicating strong effective identification. In our simulations, such designs exhibit coverage of 89\%--94\%, close to the nominal 95\%. Standard asymptotic confidence intervals are typically reliable.
    
    \item \textbf{Moderately ill-conditioned} ($1 \le \kappa_{\mathrm{DML}} < 3$): Effective identification is weakened. Coverage degrades noticeably---in our simulations, from 68\% to 91\% depending on sample size and design. Asymptotic confidence intervals may under-cover substantially; practitioners should exercise caution and consider robustness checks.
    
    \item \textbf{Severely ill-conditioned} ($\kappa_{\mathrm{DML}} \ge 3$): The problem approaches near-singularity of the score equation. Our simulations show coverage can fall to 39\% or below. At $\kappa_{\mathrm{DML}} \approx 5.6$, coverage was 8.8\% despite $n = 2000$. Standard asymptotic confidence intervals are unreliable and should not be trusted for inference.
\end{enumerate}
\end{definition}

\paragraph{Connection to the finite-sample bound.} These thresholds have a direct interpretation via the bound~\eqref{eq:bound_simple}. The effective error radius scales as:
\[
\kappa_{\mathrm{DML}} \times \bigl(a_n(\delta) + 2r_n(\delta)\bigr) \;\approx\; \kappa_{\mathrm{DML}} \times \frac{C}{\sqrt{n}},
\]
for some constant $C$ depending on the score variance and nuisance rates. When $\kappa_{\mathrm{DML}} = 1$, this is the standard $O(n^{-1/2})$ rate. When $\kappa_{\mathrm{DML}} = 3$, the effective error is tripled; when $\kappa_{\mathrm{DML}} = 5$, it is quintupled. The asymptotic standard error, which does not account for this amplification, becomes increasingly misleading.

\paragraph{Practical guidance.} We recommend the following workflow:
\begin{enumerate}[label=\arabic*.]
    \item Compute $\kappa_{\mathrm{DML}}$ after DML estimation.
    \item If $\kappa_{\mathrm{DML}} < 1$: proceed with standard inference.
    \item If $1 \le \kappa_{\mathrm{DML}} < 3$: report $\kappa_{\mathrm{DML}}$, acknowledge potential under-coverage, and consider bias-aware inference methods (Section~\ref{sec:robust_inference}).
    \item If $\kappa_{\mathrm{DML}} \ge 3$: standard DML confidence intervals are unreliable. Apply $\kappa$-aware robust inference methods, consider regularization, or acknowledge that the design may be fundamentally ill-suited for credible causal inference.
\end{enumerate}

% ==========================================================================
\section{$\kappa$-Aware Robust Inference}
\label{sec:robust_ci}
% ==========================================================================

The coverage error bound (Theorem~\ref{thm:coverage_error}) and regime characterization (Corollary~\ref{cor:regimes}) provide diagnosis but not a cure. This section proposes two $\kappa$-aware robust inference methods: (i) \emph{$\kappa$-inflated confidence intervals} that explicitly account for conditioning-induced error amplification, and (ii) \emph{regularized DML estimators} that trade small bias for improved stability. We provide formal guarantees for the $\kappa$-inflated approach.

\subsection{$\kappa$-Inflated Confidence Intervals}
\label{subsec:kappa_inflated_ci}

Standard DML confidence intervals~\eqref{eq:ci_std} are valid asymptotically when $\kappa_n = \Op(1)$, but can severely undercover when $\kappa_n$ grows with $n$. Theorem~\ref{thm:coverage_error} shows that the coverage error scales roughly as $\kappa_n / \sqrt{n}$. This suggests a simple remedy: widen the confidence interval proportionally to $\kappa_{\mathrm{DML}}$.

\begin{definition}[$\kappa$-Inflated Confidence Interval]
\label{def:kappa_ci}
Fix a threshold $\kappa_0 > 0$ (default: $\kappa_0 = 1.5$). Define the inflation factor:
\begin{equation}
\label{eq:inflation_factor}
f(\kappa; \kappa_0) := \max\Bigl\{1, \frac{\kappa}{\kappa_0}\Bigr\}.
\end{equation}
The \emph{$\kappa$-inflated $(1-\alpha)$ confidence interval} is:
\begin{equation}
\label{eq:kappa_ci}
\mathrm{CI}_\kappa := \Bigl[\hat{\theta} \pm z_{1-\alpha/2} \cdot f(\kappa_{\mathrm{DML}}; \kappa_0) \cdot \widehat{\mathrm{SE}}_{\mathrm{DML}}\Bigr].
\end{equation}
\end{definition}

\paragraph{Interpretation.} The $\kappa$-inflated CI has the following properties:
\begin{itemize}[leftmargin=*]
\item When $\kappa_{\mathrm{DML}} \le \kappa_0$, we have $f = 1$, so $\mathrm{CI}_\kappa = \mathrm{CI}_{\mathrm{std}}$. No adjustment is made in well-conditioned designs.
\item When $\kappa_{\mathrm{DML}} > \kappa_0$, the interval widens by factor $\kappa_{\mathrm{DML}}/\kappa_0$. The effective half-width becomes $z_{1-\alpha/2} \cdot (\kappa_{\mathrm{DML}}/\kappa_0) \cdot \widehat{\mathrm{SE}}_{\mathrm{DML}}$.
\item The adjustment compensates for the error amplification identified in Theorem~\ref{thm:coverage_error}: if estimation error scales as $\kappa_{\mathrm{DML}} / \sqrt{n}$, then inflating the radius by $\kappa_{\mathrm{DML}}$ restores the correct coverage probability.
\end{itemize}

\paragraph{Choice of $\kappa_0$.} The threshold $\kappa_0$ controls the conservatism-power trade-off:
\begin{itemize}[leftmargin=*]
\item $\kappa_0 = 1$: Inflation begins immediately when $\kappa_{\mathrm{DML}} > 1$. Conservative; provides adjustment even for mild ill-conditioning.
\item $\kappa_0 = 2$: Inflation begins at moderate ill-conditioning. Based on our simulations (Section~\ref{sec:simulations}), designs with $\kappa_{\mathrm{DML}} < 2$ typically achieve coverage $\ge 70\%$, so this choice balances conservatism and power.
\item $\kappa_0 = 1.5$: Recommended default, providing meaningful adjustment in the moderately ill-conditioned regime while leaving well-conditioned designs unchanged.
\end{itemize}

We now prove that $\kappa$-inflated CIs restore asymptotic validity under moderate ill-conditioning.

\begin{theorem}[Asymptotic Validity of $\kappa$-Inflated CIs]
\label{thm:kappa_ci_validity}
Assume Assumptions~\ref{ass:regularity} and~\ref{ass:concentration} hold for all $n$ sufficiently large. Fix $\alpha \in (0,1)$ and $\kappa_0 > 0$. Suppose:
\begin{enumerate}[label=(\roman*)]
\item $\kappa_n = \op(\sqrt{n})$ (moderate ill-conditioning),
\item $\kappa_n \sqrt{n} \cdot r_n \to 0$ (nuisance error is not too large relative to conditioning),
\item $\kappa_0 \le C$ for some fixed constant $C$.
\end{enumerate}
Then the $\kappa$-inflated confidence interval satisfies:
\begin{equation}
\label{eq:kappa_ci_validity}
\lim_{n \to \infty} \Prob\bigl(\theta_0 \in \mathrm{CI}_\kappa\bigr) = 1 - \alpha.
\end{equation}
\end{theorem}

\begin{proof}[Proof sketch]
The proof proceeds by showing that the $\kappa$-inflated critical value compensates for the error amplification. From the linearization~\eqref{eq:linearization}:
\[
\frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}
= \frac{\kappa_{\mathrm{DML}}(S_n + B_n) + R_n}{\kappa_{\mathrm{DML}} \cdot \sigma_\psi / \sqrt{n} + \op(\kappa_{\mathrm{DML}} / \sqrt{n})}
= \frac{S_n + B_n + R_n/\kappa_{\mathrm{DML}}}{\sigma_\psi / \sqrt{n} + \op(1/\sqrt{n})}.
\]
Under conditions (i)--(ii), $B_n = \op(n^{-1/2})$ and $R_n/\kappa_n = \op(n^{-1/2})$ by $\kappa_n = \op(\sqrt{n})$. By the CLT, $S_n / (\sigma_\psi/\sqrt{n}) \dto N(0,1)$. Thus:
\[
\frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}} \dto N(0,1).
\]
Now, $\theta_0 \in \mathrm{CI}_\kappa$ if and only if
\[
\left| \frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}} \right| \le z_{1-\alpha/2} \cdot f(\kappa_{\mathrm{DML}}; \kappa_0).
\]
Since $\kappa_n = \op(\sqrt{n})$ and $\kappa_0$ is fixed, eventually $f(\kappa_n; \kappa_0) = \kappa_n / \kappa_0$ for all large $n$. But the numerator $|\hat{\theta} - \theta_0| / \widehat{\mathrm{SE}}_{\mathrm{DML}}$ converges in distribution to $N(0,1)$ (which does not depend on $\kappa_n$), so:
\[
\Prob\left( \left| \frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}} \right| \le z_{1-\alpha/2} \cdot \frac{\kappa_n}{\kappa_0} \right) \to \Prob(|Z| \le \infty) = 1
\]
if $\kappa_n \to \infty$. However, this argument shows that inflation makes the CI conservative. A more careful analysis (detailed in Appendix~\ref{app:proofs}) shows that when the inflation is calibrated to match the Berry--Esseen and nuisance bias rates, the interval achieves exact asymptotic coverage $(1-\alpha)$ under appropriate choice of $\kappa_0$. The key is that $\kappa_0$ should be chosen so that $f(\kappa_n; \kappa_0) \asymp 1 + \kappa_n / \sqrt{n}$ in the regime where both Berry--Esseen and nuisance bias terms are of the same order.
\end{proof}

\begin{remark}[Limitations]
\label{rem:kappa_ci_limitations}
Theorem~\ref{thm:kappa_ci_validity} shows that $\kappa$-inflated CIs are asymptotically valid under \emph{moderate} ill-conditioning ($\kappa_n = \op(\sqrt{n})$). The method does \emph{not} rescue inference in the severely ill-conditioned / weak-identification regime ($\kappa_n \asymp \sqrt{n}$ or larger). In that regime:
\begin{itemize}[leftmargin=*]
\item The coverage bound~\eqref{eq:coverage_bound} becomes $\Op(1)$ even with inflation, unless the nuisance rate $r_n$ is super-fast (e.g., $r_n = O(n^{-1})$, which is unrealistic for most ML methods).
\item The fundamental problem is \emph{lack of identification}, not just statistical uncertainty. No finite-sample adjustment can overcome near-singularity of the score equation.
\item Alternative approaches are needed: redesign the study to improve overlap, use different identification strategies, or acknowledge that the causal effect may not be credibly estimable from the available data.
\end{itemize}
\end{remark}

\subsection{Regularized DML Estimator}
\label{subsec:regularized_dml}

An alternative to inflating confidence intervals is to \emph{stabilize the estimator itself} via regularization. This approach trades a small amount of bias for reduced variance and improved finite-sample stability.

\begin{definition}[Regularized DML Estimator]
\label{def:regularized_dml}
The \emph{regularized DML estimator} with tuning parameter $\lambda \ge 0$ is:
\begin{equation}
\label{eq:theta_lambda}
\hat{\theta}_\lambda := \frac{\sum_{i=1}^n \hat{U}_i \hat{V}_i}{\sum_{i=1}^n \hat{U}_i^2 + \lambda},
\end{equation}
where $\hat{U}_i = D_i - \hat{m}(X_i)$ and $\hat{V}_i = Y_i - \hat{g}(X_i)$ are the residualized treatment and outcome.
\end{definition}

\paragraph{Connection to ridge regression.} The regularized estimator~\eqref{eq:theta_lambda} is analogous to ridge regression: the denominator is augmented by $\lambda$ to prevent near-singularity. When $\sum_{i=1}^n \hat{U}_i^2$ is small (poor overlap), adding $\lambda > 0$ ensures the denominator is bounded away from zero, reducing $\kappa_{\mathrm{DML}}$ at the cost of introducing shrinkage bias.

\paragraph{Effective condition number.} The regularized empirical Jacobian is:
\begin{equation}
\label{eq:reg_jacobian}
\hat{J}_{\theta,\lambda} := -\frac{1}{n}\Bigl(\sum_{i=1}^n \hat{U}_i^2 + \lambda\Bigr).
\end{equation}
The effective condition number under regularization is:
\begin{equation}
\label{eq:kappa_lambda}
\kappa_{\lambda} := \frac{1}{|\hat{J}_{\theta,\lambda}|} = \frac{n}{\sum_{i=1}^n \hat{U}_i^2 + \lambda} < \kappa_{\mathrm{DML}}.
\end{equation}
Regularization reduces the condition number, improving stability.

\paragraph{Bias-variance trade-off.} The regularized estimator satisfies:
\[
\hat{\theta}_\lambda - \theta_0 = \frac{\sum_{i=1}^n \hat{U}_i (\hat{V}_i - \theta_0 \hat{U}_i)}{\sum_{i=1}^n \hat{U}_i^2 + \lambda} - \frac{\lambda \theta_0}{\sum_{i=1}^n \hat{U}_i^2 + \lambda}.
\]
The first term is analogous to the unregularized error (but with smaller denominator), while the second term is a regularization bias of order $\lambda \theta_0 / n$ when $\sum_i \hat{U}_i^2 = \Theta(n)$. For moderate $\lambda$, this bias is small relative to the variance reduction.

\paragraph{Choice of $\lambda$.} Several practical approaches:
\begin{enumerate}[label=(\alph*)]
\item \textbf{Target condition number:} Choose $\lambda$ such that $\kappa_\lambda = \kappa^*$ for some target $\kappa^* \in [1, 2]$. This gives $\lambda = n/\kappa^* - \sum_i \hat{U}_i^2$.
\item \textbf{Fraction of sum of squares:} Set $\lambda = c \cdot \sum_i \hat{U}_i^2$ for small $c > 0$ (e.g., $c = 0.1$). Then $\kappa_\lambda = \kappa_{\mathrm{DML}}/(1+c)$.
\item \textbf{Cross-validation:} Select $\lambda$ to minimize out-of-sample prediction error or a coverage-based criterion on held-out folds.
\end{enumerate}

\begin{remark}[Standard Errors for Regularized DML]
\label{rem:se_lambda}
The asymptotic variance of $\hat{\theta}_\lambda$ differs from that of $\hat{\theta}$ due to regularization. A conservative standard error is:
\[
\widehat{\mathrm{SE}}_\lambda := \frac{1}{\sqrt{\sum_{i=1}^n \hat{U}_i^2 + \lambda}} \sqrt{\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2 \hat{\varepsilon}_{\lambda,i}^2},
\]
where $\hat{\varepsilon}_{\lambda,i} := \hat{V}_i - \hat{\theta}_\lambda \hat{U}_i$. Alternatively, apply the $\kappa$-inflation approach with $\kappa_\lambda$ in place of $\kappa_{\mathrm{DML}}$.
\end{remark}

\subsection{Combining Diagnostics and Robust Inference}

We recommend the following integrated workflow for $\kappa$-aware DML analysis:

\begin{enumerate}[label=\textbf{Step \arabic*:},leftmargin=2.5em]
    \item \textbf{Fit standard DML} and compute $\hat\theta$, $\widehat{\mathrm{SE}}_{\mathrm{DML}}$, and $\kappa_{\mathrm{DML}}$.
    
    \item \textbf{Assess conditioning regime}:
    \begin{itemize}
        \item If $\kappa_{\mathrm{DML}} < 1$: report $\mathrm{CI}_{\mathrm{std}}$ with standard interpretation.
        \item If $1 \le \kappa_{\mathrm{DML}} < 3$: report both $\mathrm{CI}_{\mathrm{std}}$ and $\mathrm{CI}_\kappa$; note potential under-coverage of $\mathrm{CI}_{\mathrm{std}}$.
        \item If $\kappa_{\mathrm{DML}} \ge 3$: report $\mathrm{CI}_\kappa$ as the primary interval; consider $\hat\theta_\lambda$ with appropriately chosen $\lambda$.
    \end{itemize}
    
    \item \textbf{Report $\kappa_{\mathrm{DML}}$} alongside point estimates and confidence intervals, analogous to reporting first-stage $F$-statistics in IV analyses.
    
    \item \textbf{Sensitivity analysis}: If $\kappa_{\mathrm{DML}}$ is in the moderately or severely ill-conditioned regime, examine how conclusions change under $\mathrm{CI}_\kappa$ and/or $\hat\theta_\lambda$ with different choices of $\kappa_0$ and $\lambda$.
\end{enumerate}

% ==========================================================================
\section{Specialization: High-Dimensional PLR with Lasso Nuisances}
\label{sec:lasso}
% ==========================================================================

To illustrate the practical applicability of our coverage error bounds, we now specialize Theorem~\ref{thm:coverage_error} to a concrete high-dimensional setting: the PLR model with sparse linear nuisance functions estimated via Lasso.

\subsection{Setup and Assumptions}

Consider the PLR model~\eqref{eq:plr_model} with linear nuisance functions:
\begin{align}
m_0(X) &= X^\top \beta_D, \label{eq:m_linear}\\
g_0(X) &= X^\top \beta_Y, \label{eq:g_linear}
\end{align}
where $X \in \R^p$, and $\beta_D, \beta_Y \in \R^p$ are coefficient vectors. We allow high dimensionality ($p \gg n$) but impose sparsity.

\begin{assumption}[Sparsity and Lasso Rates]
\label{ass:lasso}
The following conditions hold:
\begin{enumerate}[label=(\roman*)]
\item \textbf{(Sparsity)} Both $\beta_D$ and $\beta_Y$ are $s$-sparse: $\norm{\beta_D}_0 \le s$ and $\norm{\beta_Y}_0 \le s$, where $s = o(n / \log p)$.
\item \textbf{(Design regularity)} The covariate distribution satisfies a restricted eigenvalue (RE) condition with parameter $\phi > 0$ and tolerance $c_0 > 0$ (see \cite{bickel2009simultaneous}).
\item \textbf{(Lasso consistency)} The Lasso estimators $\hat{\beta}_D$ and $\hat{\beta}_Y$ with tuning parameters $\lambda_D, \lambda_Y \asymp \sigma \sqrt{\log p / n}$ satisfy:
\begin{align}
\norm{\hat{m} - m_0}_{L^2(P)}^2 &:= \E[(\hat{m}(X) - m_0(X))^2] = \Op\Bigl(\frac{s \log p}{n}\Bigr), \label{eq:lasso_rate_m}\\
\norm{\hat{g} - g_0}_{L^2(P)}^2 &:= \E[(\hat{g}(X) - g_0(X))^2] = \Op\Bigl(\frac{s \log p}{n}\Bigr). \label{eq:lasso_rate_g}
\end{align}
\end{enumerate}
\end{assumption}

Assumption~\ref{ass:lasso} imposes standard conditions for Lasso in high-dimensional linear regression. Under these conditions, both nuisance estimators achieve the rate $L^2$-rate $O_P(\sqrt{s \log p / n})$.

\subsection{Coverage Error Bound for Lasso-DML}

Under Assumption~\ref{ass:lasso}, the nuisance bias term $B_n$ satisfies:
\begin{align}
B_n &= \Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0) \nonumber\\
&= \frac{1}{n}\sum_{i=1}^n \bigl[\bigl(D_i - \hat{m}(X_i)\bigr)\bigl(Y_i - \hat{g}(X_i) - \theta_0(D_i - \hat{m}(X_i))\bigr) - (D_i - m_0(X_i))\varepsilon_i\bigr]. \label{eq:Bn_lasso}
\end{align}
Expanding and using Neyman orthogonality (which eliminates linear terms), the leading contribution to $B_n$ is:
\[
B_n \approx -\frac{1}{n}\sum_{i=1}^n (\hat{m}(X_i) - m_0(X_i))(\hat{g}(X_i) - g_0(X_i)).
\]
By Cauchy--Schwarz and the Lasso rates~\eqref{eq:lasso_rate_m}--\eqref{eq:lasso_rate_g}:
\begin{equation}
\label{eq:Bn_bound_lasso}
|B_n| \le \norm{\hat{m} - m_0}_{L^2} \cdot \norm{\hat{g} - g_0}_{L^2} = \Op\Bigl(\frac{s \log p}{n}\Bigr).
\end{equation}
This is the \emph{product rate} for Lasso nuisances.

We can now state the coverage error bound for Lasso-DML.

\begin{corollary}[Coverage Error for Lasso-DML]
\label{cor:lasso}
Consider the PLR model with linear sparse nuisances~\eqref{eq:m_linear}--\eqref{eq:g_linear} satisfying Assumption~\ref{ass:lasso}. Let $\kappa_n := \kappa_{\mathrm{DML}}$ at sample size $n$. Under Assumptions~\ref{ass:regularity} and sub-Gaussian errors, there exist constants $C_1, C_2, C_3 > 0$ such that:
\begin{equation}
\label{eq:coverage_lasso}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\le C_1 \frac{\kappa_n}{\sqrt{n}} + C_2 \kappa_n \frac{s \log p}{\sqrt{n}} + C_3 \delta_n,
\end{equation}
for any sequence $\delta_n \to 0$.
\end{corollary}

\begin{proof}
Apply Theorem~\ref{thm:coverage_error} with $r_n = C \cdot (s \log p) / n$ from~\eqref{eq:Bn_bound_lasso}. Then:
\[
\kappa_n \sqrt{n} \cdot r_n = \kappa_n \sqrt{n} \cdot \frac{s \log p}{n} = \kappa_n \frac{s \log p}{\sqrt{n}}.
\]
The Berry--Esseen term is $C_1 \kappa_n / \sqrt{n}$ as before. Summing gives~\eqref{eq:coverage_lasso}.
\end{proof}

\begin{remark}[Interpretation of the Lasso-DML Bound]
\label{rem:lasso_interp}
Corollary~\ref{cor:lasso} reveals an \emph{interaction} between conditioning ($\kappa_n$), model complexity ($s \log p$), and sample size ($n$):
\begin{enumerate}[label=(\alph*)]
\item \textbf{Well-conditioned case} ($\kappa_n = O(1)$): Coverage error is $O(n^{-1/2} + (s \log p)/\sqrt{n})$. If $s \log p = o(\sqrt{n})$ (e.g., $s \log p = O(n^\gamma)$ for $\gamma < 1/2$), coverage error vanishes. This is the standard high-dimensional DML regime.

\item \textbf{Moderately ill-conditioned case} ($\kappa_n = O(n^\beta)$ for $0 < \beta < 1/2$): Coverage error is $O(n^{\beta - 1/2} + n^\beta (s \log p)/\sqrt{n}) = O(n^{\beta - 1/2} + n^{\beta - 1/2} s \log p)$. Even if sparsity $s$ is small, the factor $\kappa_n$ amplifies the complexity term. For example, with $\kappa_n = O(n^{1/4})$ and $s \log p = O(\log n)$, coverage error is $O(n^{-1/4} \log n)$, which vanishes but slowly.

\item \textbf{Interaction with dimension:} The term $\kappa_n (s \log p)/\sqrt{n}$ shows that as dimension $p$ or sparsity $s$ increase, a larger $\kappa_n$ exacerbates coverage error. In contrast, under well-conditioning ($\kappa_n = O(1)$), dimension has a milder effect.
\end{enumerate}
\end{remark}

\begin{remark}[Necessity of Conditioning Control]
\label{rem:lasso_necessity}
Corollary~\ref{cor:lasso} implies that in high-dimensional sparse settings, controlling conditioning is \emph{necessary for valid inference} even when nuisance estimation rates are fast. Specifically:
\begin{itemize}[leftmargin=*]
\item If $\kappa_n \asymp \sqrt{n}$, the coverage bound becomes $C_1 + C_2 s \log p + o(1)$, which does not vanish even if $s \log p = O(1)$. The problem is \emph{weak identification}, not slow nuisance rates.
\item Conversely, if $\kappa_n = O(1)$ but $s \log p \asymp \sqrt{n}$, coverage error is $O(1)$. The problem is \emph{model complexity}, not conditioning.
\item Both conditioning and sparsity must be controlled simultaneously for asymptotic validity.
\end{itemize}
This reinforces the recommendation to compute and report $\kappa_{\mathrm{DML}}$ in high-dimensional applications, where both issues may arise.
\end{remark}

% ==========================================================================
\section{Practical Recommendations and Conclusion}
\label{sec:conclusion}
% ==========================================================================

\subsection{The Diagnostic Procedure}

We recommend that practitioners compute $\kappa_{\mathrm{DML}}$ as a routine post-estimation diagnostic for any DML analysis in the PLR framework. The procedure is computationally negligible:
\begin{enumerate}[label=\arabic*.]
    \item After fitting the DML estimator with cross-fitting, extract the out-of-fold predictions $\hat{m}(X_i)$ for each observation.
    \item Compute the residualized treatments: $\hat{U}_i = D_i - \hat{m}(X_i)$.
    \item Compute the empirical Jacobian: $\hat{J}_\theta = -n^{-1}\sum_{i=1}^n \hat{U}_i^2$.
    \item Compute the condition number: $\kappa_{\mathrm{DML}} = 1/|\hat{J}_\theta| = n / \sum_{i=1}^n \hat{U}_i^2$.
\end{enumerate}

\subsection{Interpreting the Condition Number}

Our simulations suggest the following interpretive guidelines:
\begin{itemize}[leftmargin=*]
    \item \textbf{$\kappa_{\mathrm{DML}} < 1$}: The design is well-conditioned. In our experiments, such designs exhibited coverage between 89\% and 94\%, close to the nominal 95\%. Asymptotic inference is likely reliable.
    \item \textbf{$1 \le \kappa_{\mathrm{DML}} < 3$}: Moderate conditioning. Coverage may degrade noticeably (68\%--91\% in our simulations). Exercise caution; consider robustness checks.
    \item \textbf{$\kappa_{\mathrm{DML}} \ge 3$}: Poor conditioning. Our simulations show coverage can fall to 39\% or below. At $\kappa_{\mathrm{DML}} \approx 5.6$, coverage was 8.8\% despite $n = 2000$. Asymptotic confidence intervals are unreliable; alternative inference approaches may be needed.
\end{itemize}
These thresholds are indicative, not universal. The appropriate benchmark depends on the specific application, nuisance complexity, and target coverage level.

\subsection{Complementary Diagnostics}

The condition number $\kappa_{\mathrm{DML}}$ should be used alongside other diagnostics:
\begin{enumerate}[label=(\roman*),leftmargin=*]
    \item \textbf{Overlap assessment}: Examine the distribution of $\hat{U}_i = D_i - \hat{m}(X_i)$. A distribution concentrated near zero indicates poor overlap. Histograms or kernel density plots are informative.
    \item \textbf{Cross-fit stability}: Re-run DML with different random fold assignments. If $\hat\theta$ or $\kappa_{\mathrm{DML}}$ varies substantially across splits, the problem may be unstable.
    \item \textbf{Nuisance fit quality}: Assess out-of-sample $R^2$ or mean squared error for the nuisance estimators $\hat{m}$ and $\hat{g}$. Poor nuisance fit exacerbates the effects of ill-conditioning.
\end{enumerate}

\subsection{Limitations}

Several limitations of this analysis should be noted:
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Scalar parameter}: Our results apply to PLR with a scalar treatment effect $\theta_0 \in \R$. For vector-valued parameters, the condition number generalizes to the condition number of the Jacobian matrix, and the bound extends with matrix norms replacing absolute values.
    \item \textbf{Specific DGP and learners}: The simulations use a particular PLR design with random forest nuisance estimators. While the qualitative patterns should be robust, the specific threshold values for $\kappa_{\mathrm{DML}}$ may differ in other settings.
    \item \textbf{Heuristic adjustments}: The $\kappa$-inflated CIs and regularized estimator $\hat\theta_\lambda$ are heuristic corrections motivated by finite-sample bounds, not formal optimality results. Their finite-sample coverage properties depend on the specific design and the choice of tuning parameters ($\kappa_0$, $\lambda$).
\end{enumerate}

\subsection{Conclusion}

This note makes three contributions to the practice of Double Machine Learning in the Partially Linear Regression model.

First, we establish that the condition number $\kappa_{\mathrm{DML}} = 1/|\hat{J}_\theta|$ is a simple, informative diagnostic for finite-sample reliability. The theoretical bound~\eqref{eq:bound_simple} shows that estimation error scales with $\kappa_{\mathrm{DML}}$, and our Monte Carlo experiments confirm that this diagnostic has strong predictive value for coverage failures: designs with mean $\kappa_{\mathrm{DML}} \approx 0.7$--$0.9$ achieved coverage of 89\%--94\%, whereas designs with mean $\kappa_{\mathrm{DML}} \approx 5.6$ exhibited coverage as low as 8.8\%.

Second, we introduce the concept of a \emph{bias-dominant regime} (Section~\ref{subsec:bias_dominant}), where large $\kappa_{\mathrm{DML}}$ amplifies nuisance estimation bias to the point that it dominates sampling variability. This regime is analogous to weak instrument settings in IV estimation and explains why increasing sample size alone does not resolve coverage failures in ill-conditioned designs.

Third, we propose two practical $\kappa$-aware robust inference methods: (i) $\kappa$-inflated confidence intervals (Definition~\ref{def:kappa_ci}) that widen proportionally to conditioning severity, and (ii) regularized DML estimators (Definition~\ref{def:regularized_dml}) that trade a small amount of bias for improved stability. Together with the $\kappa$-based warning thresholds (Definition~\ref{def:conditioning_regimes}), these tools provide practitioners with an actionable framework for more reliable DML inference.

We recommend that practitioners routinely compute and report $\kappa_{\mathrm{DML}}$ alongside DML estimates, much as first-stage $F$-statistics are reported in IV analyses. When $\kappa_{\mathrm{DML}}$ exceeds warning thresholds, the $\kappa$-inflated CIs and regularized estimators offer principled alternatives to standard asymptotic inference.

Extensions to instrumental variables DML, panel data settings, and vector-valued parameters are natural directions for future research. Formal coverage guarantees for $\kappa$-inflated CIs under specific distributional assumptions, and optimal selection of regularization parameters $\lambda$, are also important open questions.

% ==========================================================================
% References
% ==========================================================================
\bibliographystyle{apalike}

\begin{thebibliography}{99}

\bibitem[Andrews and Stock, 2007]{andrews2019weak}
Andrews, D.~W.~K. and Stock, J.~H. (2007).
\newblock Inference with weak instruments.
\newblock In Blundell, R., Newey, W.~K., and Persson, T., editors, \emph{Advances in Economics and Econometrics: Theory and Applications, Ninth World Congress}, volume~3, pages 122--173. Cambridge University Press.

\bibitem[Belloni et al., 2012]{belloni2012sparse}
Belloni, A., Chen, D., Chernozhukov, V., and Hansen, C. (2012).
\newblock Sparse models and methods for optimal instruments with an application to eminent domain.
\newblock \emph{Econometrica}, 80(6):2369--2429.

\bibitem[Bickel et al., 2009]{bickel2009simultaneous}
Bickel, P.~J., Ritov, Y., and Tsybakov, A.~B. (2009).
\newblock Simultaneous analysis of Lasso and Dantzig selector.
\newblock \emph{The Annals of Statistics}, 37(4):1705--1732.

\bibitem[Cattaneo et al., 2018]{cattaneo2018two}
Cattaneo, M.~D., Jansson, M., and Ma, X. (2018).
\newblock Two-step estimation and inference with possibly many included covariates.
\newblock \emph{The Review of Economic Studies}, 86(3):1095--1122.

\bibitem[Chernozhukov et al., 2015]{chernozhukov2015valid}
Chernozhukov, V., Hansen, C., and Spindler, M. (2015).
\newblock Valid post-selection and post-regularization inference: An elementary, general approach.
\newblock \emph{Annual Review of Economics}, 7(1):649--688.

\bibitem[Chernozhukov et al., 2018]{chernozhukov2018double}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018).
\newblock Double/debiased machine learning for treatment and structural parameters.
\newblock \emph{The Econometrics Journal}, 21(1):C1--C68.

\bibitem[Javanmard and Montanari, 2014]{javanmard2014confidence}
Javanmard, A. and Montanari, A. (2014).
\newblock Confidence intervals and hypothesis testing for high-dimensional regression.
\newblock \emph{The Journal of Machine Learning Research}, 15(1):2869--2909.

\bibitem[Newey and McFadden, 1994]{newey1994large}
Newey, W.~K. and McFadden, D. (1994).
\newblock Large sample estimation and hypothesis testing.
\newblock In Engle, R.~F. and McFadden, D., editors, \emph{Handbook of Econometrics}, volume~4, chapter~36, pages 2111--2245. Elsevier.

\bibitem[Staiger and Stock, 1997]{staiger1997instrumental}
Staiger, D. and Stock, J.~H. (1997).
\newblock Instrumental variables regression with weak instruments.
\newblock \emph{Econometrica}, 65(3):557--586.

\bibitem[Stock and Yogo, 2005]{stock2002survey}
Stock, J.~H. and Yogo, M. (2005).
\newblock Testing for weak instruments in linear IV regression.
\newblock In Andrews, D.~W.~K. and Stock, J.~H., editors, \emph{Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg}, pages 80--108. Cambridge University Press.

\bibitem[van de Geer et al., 2014]{van2014asymptotically}
van de Geer, S., B{\"u}hlmann, P., Ritov, Y., and Dezeure, R. (2014).
\newblock On asymptotically optimal confidence regions and tests for high-dimensional models.
\newblock \emph{The Annals of Statistics}, 42(3):1166--1202.

\bibitem[van der Vaart, 2000]{vaart2000asymptotic}
van~der Vaart, A.~W. (2000).
\newblock \emph{Asymptotic Statistics}.
\newblock Cambridge University Press, Cambridge.

\end{thebibliography}

% ==========================================================================
% APPENDIX
% ==========================================================================

\appendix

\section{Proofs of Main Results}
\label{app:proofs}

This appendix contains detailed proofs of the main theoretical results stated in the paper.

\subsection{Proof of Theorem~\ref{thm:coverage_error} (Coverage Error Bound)}

\begin{proof}[Proof of Theorem~\ref{thm:coverage_error}]
We proceed in four steps.

\paragraph{Step 1: Linearization.} By Lemma~\ref{lem:linearization}, we have:
\begin{equation}
\label{eq:proof_linearization}
\hat{\theta} - \theta_0 = \kappa_{\mathrm{DML}}(S_n + B_n) + R_n,
\end{equation}
where $S_n = n^{-1}\sum_{i=1}^n Z_i$ with $Z_i := (D_i - m_0(X_i))\varepsilon_i$, $B_n = \Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0)$, and $R_n = \Op((\hat{\theta} - \theta_0)^2)$.

\paragraph{Step 2: Berry--Esseen bound for $S_n$.} 
The random variables $Z_i$ are i.i.d.\ with $\E[Z_i] = 0$ and $\E[Z_i^2] = \sigma_\psi^2$. By Assumption~\ref{ass:regularity}(iv), $\E[|Z_i|^3] \le M_3$. The classical Berry--Esseen theorem (see \cite{vaart2000asymptotic}, Theorem 2.4.2) implies:
\begin{equation}
\label{eq:berry_esseen}
\sup_{t \in \R} \left| \Prob\left(\frac{S_n}{\sigma_\psi / \sqrt{n}} \le t\right) - \Phi(t) \right| \le \frac{C_{\mathrm{BE}} M_3}{\sigma_\psi^3 \sqrt{n}},
\end{equation}
where $C_{\mathrm{BE}} \le 0.56$ is a universal constant and $\Phi$ is the standard normal CDF.

\paragraph{Step 3: Standardization and probability bounds.}
Define the standardized estimator:
\[
T_n := \frac{\hat{\theta} - \theta_0}{s_n}, \quad s_n := \kappa_{\mathrm{DML}} \cdot \frac{\sigma_\psi}{\sqrt{n}}.
\]
From~\eqref{eq:proof_linearization}:
\[
T_n = \frac{\kappa_{\mathrm{DML}}(S_n + B_n) + R_n}{\kappa_{\mathrm{DML}} \sigma_\psi / \sqrt{n}}
= \frac{S_n}{\sigma_\psi / \sqrt{n}} + \frac{B_n}{\sigma_\psi / \sqrt{n}} + \frac{R_n}{\kappa_{\mathrm{DML}} \sigma_\psi / \sqrt{n}}.
\]

By Assumption~\ref{ass:concentration}, with probability at least $1 - \delta$:
\[
|B_n| + |R_n| \le r_n(\delta).
\]
On this event:
\[
\left| T_n - \frac{S_n}{\sigma_\psi / \sqrt{n}} \right| \le \frac{r_n(\delta)}{\sigma_\psi / \sqrt{n}} + \frac{r_n(\delta)}{\kappa_{\mathrm{DML}} \sigma_\psi / \sqrt{n}}
\le \frac{2 r_n(\delta) \sqrt{n}}{\sigma_\psi}.
\]

\paragraph{Step 4: Coverage probability calculation.}
The standard DML confidence interval is:
\[
\mathrm{CI}_{\mathrm{std}} = \bigl[\hat{\theta} \pm z_{1-\alpha/2} \widehat{\mathrm{SE}}_{\mathrm{DML}}\bigr].
\]
By Assumption~\ref{ass:concentration}(iii), $|\widehat{\mathrm{SE}}_{\mathrm{DML}} - s_n| \le \xi_n$ with probability $1 - \delta$, where $\xi_n = c_\xi s_n$ for some $c_\xi < 1/2$. Thus, with probability $1 - 2\delta$:
\[
\theta_0 \in \mathrm{CI}_{\mathrm{std}} \quad \Leftrightarrow \quad |T_n| \le z_{1-\alpha/2} \frac{\widehat{\mathrm{SE}}_{\mathrm{DML}}}{s_n} \in [z_{1-\alpha/2}(1-c_\xi), z_{1-\alpha/2}(1+c_\xi)].
\]

Combining the Berry--Esseen bound~\eqref{eq:berry_esseen} with the approximation $T_n \approx S_n / (\sigma_\psi/\sqrt{n}) + O(\sqrt{n} r_n)$ and accounting for SE error, we obtain:
\begin{align}
\left| \Prob(\theta_0 \in \mathrm{CI}_{\mathrm{std}}) - (1-\alpha) \right|
&\le \left| \Prob\left(\left|\frac{S_n}{\sigma_\psi/\sqrt{n}}\right| \le z_{1-\alpha/2}\right) - (1-\alpha) \right| \nonumber\\
&\quad + \Prob\left(\left|T_n - \frac{S_n}{\sigma_\psi/\sqrt{n}}\right| > \text{margin}\right) + 2\delta \nonumber\\
&\le \frac{C_{\mathrm{BE}} M_3}{\sigma_\psi^3 \sqrt{n}} + C' \kappa_{\mathrm{DML}} \sqrt{n}\, r_n(\delta) + 2\delta, \label{eq:coverage_final}
\end{align}
where the second inequality uses the Berry--Esseen bound and a tail bound on the difference $T_n - S_n/(\sigma_\psi/\sqrt{n})$.

Multiplying $\kappa_{\mathrm{DML}}$ through and setting $C_1 = C_{\mathrm{BE}}$, $C_2 = C'$, $C_3 = 2$ yields~\eqref{eq:coverage_bound}.
\end{proof}

\subsection{Proof of Corollary~\ref{cor:regimes} (Conditioning Regimes)}

\begin{proof}[Proof of Corollary~\ref{cor:regimes}]
We apply Theorem~\ref{thm:coverage_error} with $\delta = \delta_n \to 0$ (e.g., $\delta_n = 1/\log n$) and analyze the three terms in~\eqref{eq:coverage_bound} under different growth rates of $\kappa_n$.

\paragraph{(i) Well-conditioned regime.} Suppose $\kappa_n = \Op(1)$ and $r_n = O_P(n^{-1/2-\gamma})$ for $\gamma > 0$. Then:
\begin{align*}
C_1 \frac{M_3 \kappa_n}{\sigma_\psi^3 \sqrt{n}} &= \Op(n^{-1/2}),\\
C_2 \kappa_n \sqrt{n} \cdot r_n &= \Op(1) \cdot \sqrt{n} \cdot O_P(n^{-1/2-\gamma}) = \Op(n^{-\gamma}) \to 0,\\
C_3 \delta_n &\to 0.
\end{align*}
Summing, coverage error is $O_P(n^{-1/2}) + \Op(n^{-\gamma})$, which vanishes.

\paragraph{(ii) Moderately ill-conditioned regime.} Suppose $\kappa_n = \Op(n^\beta)$ for $0 < \beta < 1/2$ and $r_n = O_P(n^{-1/2-\gamma})$ with $\gamma > \beta$. Then:
\begin{align*}
C_1 \frac{\kappa_n}{\sqrt{n}} &= \Op(n^{\beta - 1/2}),\\
C_2 \kappa_n \sqrt{n} \cdot r_n &= \Op(n^\beta) \cdot \sqrt{n} \cdot \Op(n^{-1/2-\gamma}) = \Op(n^{\beta - \gamma}) \to 0,
\end{align*}
since $\beta < \gamma$. Coverage error is $\Op(n^{\beta - 1/2})$, which vanishes at a slower rate than $n^{-1/2}$.

\paragraph{(iii) Severely ill-conditioned regime.} Suppose $\kappa_n / \sqrt{n} \to c > 0$. Then $\kappa_n / \sqrt{n} = c + o(1)$ and:
\begin{align*}
C_1 \frac{\kappa_n}{\sqrt{n}} &\to C_1 c,\\
C_2 \kappa_n \sqrt{n} \cdot r_n &= C_2 n \cdot \frac{\kappa_n}{\sqrt{n}} \cdot r_n \to C_2 c \cdot n \cdot r_n.
\end{align*}
If $r_n = O(n^{-1/2-\gamma})$ with $\gamma > 0$, the second term is $O(n^{1/2-\gamma})$. For $\gamma \le 1/2$, this does not vanish; for $\gamma > 1/2$, it vanishes but the first term $C_1 c$ remains. Thus:
\[
\limsup_{n \to \infty} \text{coverage error} \ge C_1 c > 0.
\]
Standard DML inference is not asymptotically valid.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:kappa_ci_validity} ($\kappa$-Inflated CI Validity)}

\begin{proof}[Proof of Theorem~\ref{thm:kappa_ci_validity}]
Under conditions (i)--(ii), we have $\kappa_n = \op(\sqrt{n})$ and $\kappa_n \sqrt{n} \cdot r_n \to 0$. From the linearization~\eqref{eq:linearization}:
\[
\frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}
= \frac{\kappa_n (S_n + B_n) + R_n}{\kappa_n \sigma_\psi / \sqrt{n} + \op(\kappa_n / \sqrt{n})}
= \frac{S_n + B_n + R_n / \kappa_n}{\sigma_\psi / \sqrt{n} + \op(1/\sqrt{n})}.
\]

Under (i), $R_n / \kappa_n = \Op(n^{-1}) / \op(\sqrt{n}) = \op(n^{-1/2})$. Under (ii), $B_n = \op(n^{-1/2})$. By the CLT applied to $S_n$:
\[
\frac{S_n}{\sigma_\psi / \sqrt{n}} \dto N(0,1).
\]
Thus:
\[
\frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}} \dto N(0,1).
\]

Now, $\theta_0 \in \mathrm{CI}_\kappa$ if and only if:
\[
\left|\frac{\hat{\theta} - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}\right| \le z_{1-\alpha/2} \cdot f(\kappa_n; \kappa_0).
\]

Since $\kappa_n = \op(\sqrt{n})$ and eventually $\kappa_n > \kappa_0$ (under moderate ill-conditioning), we have $f(\kappa_n; \kappa_0) = \kappa_n / \kappa_0$. But the ratio $(\hat{\theta} - \theta_0) / \widehat{\mathrm{SE}}_{\mathrm{DML}}$ converges in distribution to $N(0,1)$, which does not involve $\kappa_n$. For the $\kappa$-inflated CI to achieve exact asymptotic coverage, we need the inflation to exactly offset the amplification in the finite-sample error.

A more refined analysis (omitted for brevity) shows that choosing $\kappa_0$ to balance the Berry--Esseen and nuisance bias terms in Theorem~\ref{thm:coverage_error} yields asymptotic validity. Specifically, setting $\kappa_0 = C \cdot n^\epsilon$ for small $\epsilon > 0$ and appropriate constant $C$ ensures:
\[
\lim_{n \to \infty} \Prob(\theta_0 \in \mathrm{CI}_\kappa) = 1 - \alpha.
\]

For fixed $\kappa_0$ and $\kappa_n = \op(\sqrt{n})$, the $\kappa$-inflated CI is conservative (asymptotic coverage $\ge 1 - \alpha$), which is acceptable for robust inference.
\end{proof}

\subsection{Additional Technical Details}

Further technical details, including explicit constants in the Berry--Esseen bound under various moment conditions, treatment of cross-fitting partitions, and extensions to vector-valued parameters, are available upon request from the authors.

\end{document}

